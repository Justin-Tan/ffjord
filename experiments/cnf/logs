/Users/justin.tian/github/ffjord/pie.md
/Users/justin.tian/github/ffjord/train_toy.py
/Users/justin.tian/github/ffjord/train_toy.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform
import lib.layers.odefunc as odefunc

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import build_model_tabular

from diagnostics.viz_toy import save_trajectory, trajectory_to_video

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)
parser.add_argument(
    "--layer_type", type=str, default="concatsquash",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument('--dims', type=str, default='64-64-64')
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')
parser.add_argument('--time_length', type=float, default=0.5)
parser.add_argument('--train_T', type=eval, default=True)
parser.add_argument("--divergence_fn", type=str, default="brute_force", choices=["brute_force", "approximate"])
parser.add_argument("--nonlinearity", type=str, default="tanh", choices=odefunc.NONLINEARITIES)

parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--bn_lag', type=float, default=0)

parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--weight_decay', type=float, default=1e-5)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=100)
parser.add_argument('--val_freq', type=int, default=100)
parser.add_argument('--log_freq', type=int, default=10)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def get_transforms(model):

    def sample_fn(z, logpz=None):
        if logpz is not None:
            return model(z, logpz, reverse=True)
        else:
            return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, 2, regularization_fns).to(device)
    if args.spectral_norm: add_spectral_norm(model)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()
        if args.spectral_norm: spectral_norm_power_iteration(model, 1)

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
            )
            loss = loss + reg_loss

        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)

        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | NFE Forward {:.0f}({:.1f})'
            ' | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg,
                nfeb_meter.val, nfeb_meter.avg, tt_meter.val, tt_meter.avg
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                test_nfe = count_nfe(model)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss, test_nfe)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

    save_traj_dir = os.path.join(args.save, 'trajectory')
    logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = toy_data.inf_train_gen(args.data, batch_size=2000)
    save_trajectory(model, data_samples, save_traj_dir, device=device)
    trajectory_to_video(save_traj_dir)

import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform
import lib.layers.odefunc as odefunc

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import build_model_tabular

from diagnostics.viz_toy import save_trajectory, trajectory_to_video

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)
parser.add_argument(
    "--layer_type", type=str, default="concatsquash",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument('--dims', type=str, default='64-64-64')
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')
parser.add_argument('--time_length', type=float, default=0.5)
parser.add_argument('--train_T', type=eval, default=True)
parser.add_argument("--divergence_fn", type=str, default="brute_force", choices=["brute_force", "approximate"])
parser.add_argument("--nonlinearity", type=str, default="tanh", choices=odefunc.NONLINEARITIES)

parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--bn_lag', type=float, default=0)

parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--weight_decay', type=float, default=1e-5)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=100)
parser.add_argument('--val_freq', type=int, default=100)
parser.add_argument('--log_freq', type=int, default=10)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')


def get_transforms(model):

    def sample_fn(z, logpz=None):
        if logpz is not None:
            return model(z, logpz, reverse=True)
        else:
            return model(z, reverse=True)

    def density_fn(x, logpx=None):
        if logpx is not None:
            return model(x, logpx, reverse=False)
        else:
            return model(x, reverse=False)

    return sample_fn, density_fn


def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss


if __name__ == '__main__':

    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, 2, regularization_fns).to(device)
    if args.spectral_norm: add_spectral_norm(model)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))

    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    end = time.time()
    best_loss = float('inf')
    model.train()
    for itr in range(1, args.niters + 1):
        optimizer.zero_grad()
        if args.spectral_norm: spectral_norm_power_iteration(model, 1)

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        if len(regularization_coeffs) > 0:
            reg_states = get_regularization(model, regularization_coeffs)
            reg_loss = sum(
                reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
            )
            loss = loss + reg_loss

        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)

        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | NFE Forward {:.0f}({:.1f})'
            ' | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg,
                nfeb_meter.val, nfeb_meter.avg, tt_meter.val, tt_meter.avg
            )
        )
        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                test_nfe = count_nfe(model)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss, test_nfe)
                logger.info(log_message)

                if test_loss.item() < best_loss:
                    best_loss = test_loss.item()
                    utils.makedirs(args.save)
                    torch.save({
                        'args': args,
                        'state_dict': model.state_dict(),
                    }, os.path.join(args.save, 'checkpt.pth'))
                model.train()

        if itr % args.viz_freq == 0:
            with torch.no_grad():
                model.eval()
                p_samples = toy_data.inf_train_gen(args.data, batch_size=2000)

                sample_fn, density_fn = get_transforms(model)

                plt.figure(figsize=(9, 3))
                visualize_transform(
                    p_samples, torch.randn, standard_normal_logprob, transform=sample_fn, inverse_transform=density_fn,
                    samples=True, npts=800, device=device
                )
                fig_filename = os.path.join(args.save, 'figs', '{:04d}.jpg'.format(itr))
                utils.makedirs(os.path.dirname(fig_filename))
                plt.savefig(fig_filename)
                plt.close()
                model.train()

        end = time.time()

    logger.info('Training has finished.')

    save_traj_dir = os.path.join(args.save, 'trajectory')
    logger.info('Plotting trajectory to {}'.format(save_traj_dir))
    data_samples = toy_data.inf_train_gen(args.data, batch_size=2000)
    save_trajectory(model, data_samples, save_traj_dir, device=device)
    trajectory_to_video(save_traj_dir)

Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

import argparse
import os
import time

import torch
import torch.optim as optim

import lib.toy_data as toy_data
import lib.utils as utils
from lib.visualize_flow import visualize_transform
import lib.layers.odefunc as odefunc

from train_misc import standard_normal_logprob
from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time
from train_misc import add_spectral_norm, spectral_norm_power_iteration
from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log
from train_misc import build_model_tabular

from diagnostics.viz_toy import save_trajectory, trajectory_to_video

SOLVERS = ["dopri5", "bdf", "rk4", "midpoint", 'adams', 'explicit_adams', 'fixed_adams']
parser = argparse.ArgumentParser('Continuous Normalizing Flow')
parser.add_argument(
    '--data', choices=['swissroll', '8gaussians', 'pinwheel', 'circles', 'moons', '2spirals', 'checkerboard', 'rings'],
    type=str, default='pinwheel'
)
parser.add_argument(
    "--layer_type", type=str, default="concatsquash",
    choices=["ignore", "concat", "concat_v2", "squash", "concatsquash", "concatcoord", "hyper", "blend"]
)
parser.add_argument('--dims', type=str, default='64-64-64')
parser.add_argument("--num_blocks", type=int, default=1, help='Number of stacked CNFs.')
parser.add_argument('--time_length', type=float, default=0.5)
parser.add_argument('--train_T', type=eval, default=True)
parser.add_argument("--divergence_fn", type=str, default="brute_force", choices=["brute_force", "approximate"])
parser.add_argument("--nonlinearity", type=str, default="tanh", choices=odefunc.NONLINEARITIES)

parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)
parser.add_argument('--atol', type=float, default=1e-5)
parser.add_argument('--rtol', type=float, default=1e-5)
parser.add_argument("--step_size", type=float, default=None, help="Optional fixed step size.")

parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])
parser.add_argument('--test_atol', type=float, default=None)
parser.add_argument('--test_rtol', type=float, default=None)

parser.add_argument('--residual', type=eval, default=False, choices=[True, False])
parser.add_argument('--rademacher', type=eval, default=False, choices=[True, False])
parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--batch_norm', type=eval, default=False, choices=[True, False])
parser.add_argument('--bn_lag', type=float, default=0)

parser.add_argument('--niters', type=int, default=10000)
parser.add_argument('--batch_size', type=int, default=100)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--lr', type=float, default=1e-3)
parser.add_argument('--weight_decay', type=float, default=1e-5)

# Track quantities
parser.add_argument('--l1int', type=float, default=None, help="int_t ||f||_1")
parser.add_argument('--l2int', type=float, default=None, help="int_t ||f||_2")
parser.add_argument('--dl2int', type=float, default=None, help="int_t ||f^T df/dt||_2")
parser.add_argument('--JFrobint', type=float, default=None, help="int_t ||df/dx||_F")
parser.add_argument('--JdiagFrobint', type=float, default=None, help="int_t ||df_i/dx_i||_F")
parser.add_argument('--JoffdiagFrobint', type=float, default=None, help="int_t ||df/dx - df_i/dx_i||_F")

parser.add_argument('--save', type=str, default='experiments/cnf')
parser.add_argument('--viz_freq', type=int, default=100)
parser.add_argument('--val_freq', type=int, default=100)
parser.add_argument('--log_freq', type=int, default=10)
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

# logger
utils.makedirs(args.save)
logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))

if args.layer_type == "blend":
    logger.info("!! Setting time_length from None to 1.0 due to use of Blend layers.")
    args.time_length = 1.0

logger.info(args)

def compute_loss(args, model, batch_size=None):
    if batch_size is None: batch_size = args.batch_size

    # load data
    # x = toy_data.inf_train_gen(args.data, batch_size=batch_size)
    x = sklearn.datasets.make_moons(n_samples=256, noise=.05)[0].astype(np.float32)
    x = torch.from_numpy(x).type(torch.float32).to(device)
    zero = torch.zeros(x.shape[0], 1).to(x)

    # transform to z
    z, delta_logp = model(x, zero)

    # compute log q(z)
    logpz = standard_normal_logprob(z).sum(1, keepdim=True)

    logpx = logpz - delta_logp
    loss = -torch.mean(logpx)
    return loss

def get_regularization_loss(model, regularization_fns, regularization_coeffs):
    if len(regularization_coeffs) > 0:
        reg_states = get_regularization(model, regularization_coeffs)
        reg_loss = sum(
            reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0
        )

    return reg_loss

def train_moons_ffjord(model, optimizer, device, logger, iterations=8000):
    print('Using device', device)

    for idx in trange(iterations, desc='Itr'):
        optimizer.zero_grad()
        if args.spectral_norm: spectral_norm_power_iteration(model, 1)

        loss = compute_loss(args, model)
        loss_meter.update(loss.item())

        reg_loss = get_regularization_loss(model, regularization_fns, regularization_coeffs)

        loss = loss + reg_loss

        total_time = count_total_time(model)
        nfe_forward = count_nfe(model)

        loss.backward()
        optimizer.step()

        nfe_total = count_nfe(model)
        nfe_backward = nfe_total - nfe_forward
        nfef_meter.update(nfe_forward)
        nfeb_meter.update(nfe_backward)
        time_meter.update(time.time() - end)
        tt_meter.update(total_time)

        log_message = (
            'Iter {:04d} | Time {:.4f}({:.4f}) | Loss {:.6f}({:.6f}) | NFE Forward {:.0f}({:.1f})'
            ' | NFE Backward {:.0f}({:.1f}) | CNF Time {:.4f}({:.4f})'.format(
                itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, nfef_meter.val, nfef_meter.avg,
                nfeb_meter.val, nfeb_meter.avg, tt_meter.val, tt_meter.avg
            )
        )

        if len(regularization_coeffs) > 0:
            log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)

        logger.info(log_message)

        if itr % args.val_freq == 0 or itr == args.niters:
            with torch.no_grad():
                model.eval()
                test_loss = compute_loss(args, model, batch_size=args.test_batch_size)
                test_nfe = count_nfe(model)
                log_message = '[TEST] Iter {:04d} | Test Loss {:.6f} | NFE {:.0f}'.format(itr, test_loss, test_nfe)
                logger.info(log_message)

        end = time.time()

    logger.info('Training has finished.')


if __name__ == '__main__':

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')
    regularization_fns, regularization_coeffs = create_regularization_fns(args)
    model = build_model_tabular(args, 2, regularization_fns).to(device)
    if args.spectral_norm: add_spectral_norm(model)
    set_cnf_options(args, model)

    logger.info(model)
    logger.info("Number of trainable parameters: {}".format(count_parameters(model)))
    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)

    time_meter = utils.RunningAverageMeter(0.93)
    loss_meter = utils.RunningAverageMeter(0.93)
    nfef_meter = utils.RunningAverageMeter(0.93)
    nfeb_meter = utils.RunningAverageMeter(0.93)
    tt_meter = utils.RunningAverageMeter(0.93)

    train_moons_ffjord(model, optimizer, device, logger, iterations=8000)
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
Iter 0000 | Time 0.5285(0.5285) | Loss 2.529887(2.529887) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0000 | Test Loss 2.530318 | NFE 20 [*]
Iter 0001 | Time 0.2413(0.5084) | Loss 2.531453(2.529997) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0002 | Time 0.2423(0.4898) | Loss 2.523504(2.529542) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0003 | Time 0.2425(0.4724) | Loss 2.522538(2.529052) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0004 | Time 0.2419(0.4563) | Loss 2.515671(2.528115) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0005 | Time 0.2453(0.4415) | Loss 2.512060(2.526991) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0006 | Time 0.2421(0.4276) | Loss 2.510421(2.525831) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0007 | Time 0.2447(0.4148) | Loss 2.506812(2.524500) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0008 | Time 0.2416(0.4027) | Loss 2.504354(2.523090) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0009 | Time 0.2423(0.3914) | Loss 2.500045(2.521477) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0010 | Time 0.2415(0.3809) | Loss 2.487307(2.519085) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0011 | Time 0.2430(0.3713) | Loss 2.485399(2.516727) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0012 | Time 0.2415(0.3622) | Loss 2.489984(2.514855) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0013 | Time 0.2417(0.3538) | Loss 2.479564(2.512384) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0014 | Time 0.2435(0.3460) | Loss 2.480891(2.510180) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0015 | Time 0.2419(0.3388) | Loss 2.467859(2.507217) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0016 | Time 0.2446(0.3322) | Loss 2.461203(2.503996) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0017 | Time 0.2444(0.3260) | Loss 2.465131(2.501276) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0018 | Time 0.2879(0.3233) | Loss 2.456325(2.498129) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0019 | Time 0.2415(0.3176) | Loss 2.447223(2.494566) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0020 | Time 0.2416(0.3123) | Loss 2.445011(2.491097) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0021 | Time 0.2419(0.3074) | Loss 2.436202(2.487254) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0022 | Time 0.2432(0.3029) | Loss 2.431407(2.483345) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0023 | Time 0.2412(0.2986) | Loss 2.421261(2.478999) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0024 | Time 0.2417(0.2946) | Loss 2.415736(2.474571) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0025 | Time 0.2410(0.2908) | Loss 2.412327(2.470214) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0026 | Time 0.2415(0.2874) | Loss 2.405075(2.465654) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0027 | Time 0.2436(0.2843) | Loss 2.394198(2.460652) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0028 | Time 0.2410(0.2813) | Loss 2.389838(2.455695) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0029 | Time 0.2409(0.2785) | Loss 2.374143(2.449986) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0030 | Time 0.2411(0.2758) | Loss 2.365455(2.444069) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0031 | Time 0.2410(0.2734) | Loss 2.353949(2.437761) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0032 | Time 0.2402(0.2711) | Loss 2.343343(2.431152) | NFE Forward 20(20.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
Iter 0033 | Time 0.3027(0.2733) | Loss 2.333784(2.424336) | NFE Forward 20(20.0) | NFE Backward 21(15.4) | CNF Time 0.5000(0.5000)
Iter 0034 | Time 0.2994(0.2751) | Loss 2.328620(2.417636) | NFE Forward 20(20.0) | NFE Backward 21(15.8) | CNF Time 0.5000(0.5000)
Iter 0035 | Time 0.2404(0.2727) | Loss 2.308554(2.410000) | NFE Forward 20(20.0) | NFE Backward 15(15.8) | CNF Time 0.5000(0.5000)
Iter 0036 | Time 0.2420(0.2705) | Loss 2.299116(2.402238) | NFE Forward 20(20.0) | NFE Backward 15(15.7) | CNF Time 0.5000(0.5000)
Iter 0037 | Time 0.3017(0.2727) | Loss 2.285582(2.394072) | NFE Forward 20(20.0) | NFE Backward 21(16.1) | CNF Time 0.5000(0.5000)
Iter 0038 | Time 0.2413(0.2705) | Loss 2.273083(2.385603) | NFE Forward 20(20.0) | NFE Backward 15(16.0) | CNF Time 0.5000(0.5000)
Iter 0039 | Time 0.3050(0.2729) | Loss 2.258676(2.376718) | NFE Forward 20(20.0) | NFE Backward 21(16.3) | CNF Time 0.5000(0.5000)
Iter 0040 | Time 0.3017(0.2749) | Loss 2.246043(2.367571) | NFE Forward 20(20.0) | NFE Backward 21(16.7) | CNF Time 0.5000(0.5000)
Iter 0041 | Time 0.3009(0.2768) | Loss 2.224101(2.357528) | NFE Forward 20(20.0) | NFE Backward 21(17.0) | CNF Time 0.5000(0.5000)
Iter 0042 | Time 0.3042(0.2787) | Loss 2.216291(2.347641) | NFE Forward 20(20.0) | NFE Backward 21(17.3) | CNF Time 0.5000(0.5000)
Iter 0043 | Time 0.3002(0.2802) | Loss 2.206149(2.337737) | NFE Forward 20(20.0) | NFE Backward 21(17.5) | CNF Time 0.5000(0.5000)
Iter 0044 | Time 0.3005(0.2816) | Loss 2.191983(2.327534) | NFE Forward 20(20.0) | NFE Backward 21(17.8) | CNF Time 0.5000(0.5000)
Iter 0045 | Time 0.3034(0.2831) | Loss 2.174825(2.316844) | NFE Forward 20(20.0) | NFE Backward 21(18.0) | CNF Time 0.5000(0.5000)
Iter 0046 | Time 0.3017(0.2844) | Loss 2.157798(2.305711) | NFE Forward 20(20.0) | NFE Backward 21(18.2) | CNF Time 0.5000(0.5000)
Iter 0047 | Time 0.3012(0.2856) | Loss 2.142013(2.294252) | NFE Forward 20(20.0) | NFE Backward 21(18.4) | CNF Time 0.5000(0.5000)
Iter 0048 | Time 0.3031(0.2868) | Loss 2.134097(2.283041) | NFE Forward 20(20.0) | NFE Backward 21(18.6) | CNF Time 0.5000(0.5000)
Iter 0049 | Time 0.3006(0.2878) | Loss 2.112658(2.271115) | NFE Forward 20(20.0) | NFE Backward 21(18.7) | CNF Time 0.5000(0.5000)
Iter 0050 | Time 0.3011(0.2887) | Loss 2.094386(2.258744) | NFE Forward 20(20.0) | NFE Backward 21(18.9) | CNF Time 0.5000(0.5000)
Iter 0051 | Time 0.3014(0.2896) | Loss 2.083880(2.246503) | NFE Forward 20(20.0) | NFE Backward 21(19.1) | CNF Time 0.5000(0.5000)
Iter 0052 | Time 0.3008(0.2904) | Loss 2.064995(2.233798) | NFE Forward 20(20.0) | NFE Backward 21(19.2) | CNF Time 0.5000(0.5000)
Iter 0053 | Time 0.3003(0.2911) | Loss 2.049453(2.220893) | NFE Forward 20(20.0) | NFE Backward 21(19.3) | CNF Time 0.5000(0.5000)
Iter 0054 | Time 0.3003(0.2917) | Loss 2.054235(2.209227) | NFE Forward 20(20.0) | NFE Backward 21(19.4) | CNF Time 0.5000(0.5000)
Iter 0055 | Time 0.3006(0.2924) | Loss 2.048623(2.197985) | NFE Forward 20(20.0) | NFE Backward 21(19.5) | CNF Time 0.5000(0.5000)
Iter 0056 | Time 0.3030(0.2931) | Loss 2.026219(2.185961) | NFE Forward 20(20.0) | NFE Backward 21(19.6) | CNF Time 0.5000(0.5000)
Iter 0057 | Time 0.3012(0.2937) | Loss 2.014956(2.173991) | NFE Forward 20(20.0) | NFE Backward 21(19.7) | CNF Time 0.5000(0.5000)
Iter 0058 | Time 0.3004(0.2942) | Loss 2.015733(2.162913) | NFE Forward 20(20.0) | NFE Backward 21(19.8) | CNF Time 0.5000(0.5000)
Iter 0059 | Time 0.3010(0.2946) | Loss 2.014957(2.152556) | NFE Forward 20(20.0) | NFE Backward 21(19.9) | CNF Time 0.5000(0.5000)
Iter 0060 | Time 0.3000(0.2950) | Loss 2.009418(2.142536) | NFE Forward 20(20.0) | NFE Backward 21(20.0) | CNF Time 0.5000(0.5000)
Iter 0061 | Time 0.3516(0.2990) | Loss 2.008560(2.133158) | NFE Forward 20(20.0) | NFE Backward 27(20.5) | CNF Time 0.5000(0.5000)
Iter 0062 | Time 0.3521(0.3027) | Loss 2.013568(2.124787) | NFE Forward 20(20.0) | NFE Backward 27(20.9) | CNF Time 0.5000(0.5000)
Iter 0063 | Time 0.3527(0.3062) | Loss 2.013913(2.117025) | NFE Forward 20(20.0) | NFE Backward 27(21.4) | CNF Time 0.5000(0.5000)
Iter 0064 | Time 0.3511(0.3093) | Loss 2.001564(2.108943) | NFE Forward 20(20.0) | NFE Backward 27(21.8) | CNF Time 0.5000(0.5000)
Iter 0065 | Time 0.3506(0.3122) | Loss 1.992359(2.100782) | NFE Forward 20(20.0) | NFE Backward 27(22.1) | CNF Time 0.5000(0.5000)
Iter 0066 | Time 0.4043(0.3187) | Loss 1.985300(2.092699) | NFE Forward 20(20.0) | NFE Backward 33(22.9) | CNF Time 0.5000(0.5000)
Iter 0067 | Time 0.3532(0.3211) | Loss 1.985532(2.085197) | NFE Forward 20(20.0) | NFE Backward 27(23.2) | CNF Time 0.5000(0.5000)
Iter 0068 | Time 0.3531(0.3233) | Loss 1.991361(2.078628) | NFE Forward 20(20.0) | NFE Backward 27(23.4) | CNF Time 0.5000(0.5000)
Iter 0069 | Time 0.3531(0.3254) | Loss 1.985844(2.072133) | NFE Forward 20(20.0) | NFE Backward 27(23.7) | CNF Time 0.5000(0.5000)
Iter 0070 | Time 0.4056(0.3310) | Loss 1.976867(2.065465) | NFE Forward 20(20.0) | NFE Backward 33(24.3) | CNF Time 0.5000(0.5000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Number of trainable parameters: 9225
Iter 0000 | Time 0.5008(0.5008) | Loss 2.469780(2.469780) | NFE Forward 14(14.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0000 | Test Loss 2.452884 | NFE 14 [*]
Iter 0100 | Time 0.3052(0.3097) | Loss 1.917452(1.931272) | NFE Forward 20(20.6) | NFE Backward 21(21.4) | CNF Time 0.5000(0.5000)
[TEST] Iter 0100 | Test Loss 1.937900 | NFE 20 [*]
Iter 0200 | Time 0.4678(0.4504) | Loss 1.795152(1.838358) | NFE Forward 32(27.0) | NFE Backward 33(33.3) | CNF Time 0.5000(0.5000)
[TEST] Iter 0200 | Test Loss 1.800927 | NFE 32 [*]
Iter 0300 | Time 0.4999(0.4858) | Loss 1.487526(1.513265) | NFE Forward 38(33.6) | NFE Backward 33(33.4) | CNF Time 0.5000(0.5000)
[TEST] Iter 0300 | Test Loss 1.496343 | NFE 32 [*]
Iter 0400 | Time 0.5867(0.5681) | Loss 1.084885(1.180337) | NFE Forward 44(39.6) | NFE Backward 39(39.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0400 | Test Loss 1.097025 | NFE 38 [*]
Iter 0500 | Time 0.6756(0.6110) | Loss 0.975273(0.915310) | NFE Forward 50(44.6) | NFE Backward 45(40.9) | CNF Time 0.5000(0.5000)
[TEST] Iter 0500 | Test Loss 0.863249 | NFE 44 [*]
Iter 0600 | Time 0.6746(0.6716) | Loss 0.757830(0.746493) | NFE Forward 50(49.1) | NFE Backward 45(45.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0600 | Test Loss 0.746526 | NFE 50 [*]
Iter 0700 | Time 0.7304(0.6847) | Loss 0.576282(0.631269) | NFE Forward 62(50.5) | NFE Backward 45(45.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 0700 | Test Loss 0.588902 | NFE 50 [*]
Iter 0800 | Time 0.6748(0.6886) | Loss 0.553892(0.585663) | NFE Forward 50(47.3) | NFE Backward 45(47.9) | CNF Time 0.5000(0.5000)
[TEST] Iter 0800 | Test Loss 0.568080 | NFE 50 [*]
Iter 0900 | Time 0.6997(0.6785) | Loss 0.482927(0.517353) | NFE Forward 44(43.2) | NFE Backward 51(48.7) | CNF Time 0.5000(0.5000)
[TEST] Iter 0900 | Test Loss 0.459123 | NFE 44 [*]
Iter 1000 | Time 0.6782(0.6895) | Loss 0.490808(0.504459) | NFE Forward 38(42.2) | NFE Backward 51(50.1) | CNF Time 0.5000(0.5000)
[TEST] Iter 1000 | Test Loss 0.434010 | NFE 50 [*]
Iter 1100 | Time 0.7034(0.6906) | Loss 0.497605(0.492266) | NFE Forward 44(44.5) | NFE Backward 51(49.2) | CNF Time 0.5000(0.5000)
[TEST] Iter 1100 | Test Loss 0.579045 | NFE 44 []
Iter 1200 | Time 0.6748(0.7148) | Loss 0.469059(0.458977) | NFE Forward 50(48.1) | NFE Backward 45(50.1) | CNF Time 0.5000(0.5000)
[TEST] Iter 1200 | Test Loss 0.458606 | NFE 50 []
Iter 1300 | Time 0.7300(0.7299) | Loss 0.517400(0.433563) | NFE Forward 50(49.9) | NFE Backward 51(50.8) | CNF Time 0.5000(0.5000)
[TEST] Iter 1300 | Test Loss 0.405325 | NFE 50 [*]
Iter 1400 | Time 0.7384(0.7454) | Loss 0.514947(0.419845) | NFE Forward 50(50.5) | NFE Backward 51(51.9) | CNF Time 0.5000(0.5000)
[TEST] Iter 1400 | Test Loss 0.427990 | NFE 50 []
Iter 1500 | Time 0.8451(0.8190) | Loss 0.399838(0.410558) | NFE Forward 50(58.5) | NFE Backward 63(56.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 1500 | Test Loss 0.372821 | NFE 50 [*]
Iter 1600 | Time 0.7852(0.8376) | Loss 0.379835(0.405688) | NFE Forward 62(63.0) | NFE Backward 51(55.6) | CNF Time 0.5000(0.5000)
[TEST] Iter 1600 | Test Loss 0.448948 | NFE 56 []
Iter 1700 | Time 0.8420(0.8342) | Loss 0.368198(0.415085) | NFE Forward 62(58.3) | NFE Backward 57(57.8) | CNF Time 0.5000(0.5000)
[TEST] Iter 1700 | Test Loss 0.544957 | NFE 56 []
Iter 1800 | Time 0.8174(0.8431) | Loss 0.387623(0.393520) | NFE Forward 68(60.5) | NFE Backward 51(57.6) | CNF Time 0.5000(0.5000)
[TEST] Iter 1800 | Test Loss 0.340418 | NFE 56 [*]
Iter 1900 | Time 0.9304(0.8849) | Loss 0.447501(0.416336) | NFE Forward 68(65.4) | NFE Backward 63(59.8) | CNF Time 0.5000(0.5000)
[TEST] Iter 1900 | Test Loss 0.389085 | NFE 68 []
Iter 2000 | Time 0.8666(0.9104) | Loss 0.361523(0.384973) | NFE Forward 68(67.3) | NFE Backward 57(61.7) | CNF Time 0.5000(0.5000)
[TEST] Iter 2000 | Test Loss 0.391658 | NFE 62 []
Iter 2100 | Time 0.9553(0.9442) | Loss 0.353809(0.377240) | NFE Forward 62(65.8) | NFE Backward 69(65.6) | CNF Time 0.5000(0.5000)
[TEST] Iter 2100 | Test Loss 0.407855 | NFE 68 []
Iter 2200 | Time 0.9349(0.9778) | Loss 0.303890(0.359295) | NFE Forward 56(72.2) | NFE Backward 69(65.9) | CNF Time 0.5000(0.5000)
[TEST] Iter 2200 | Test Loss 0.404291 | NFE 80 []
Iter 2300 | Time 1.0704(1.0169) | Loss 0.410951(0.379417) | NFE Forward 74(72.8) | NFE Backward 75(69.6) | CNF Time 0.5000(0.5000)
[TEST] Iter 2300 | Test Loss 0.332684 | NFE 68 [*]
Iter 2400 | Time 1.1572(1.0627) | Loss 0.409081(0.363397) | NFE Forward 80(76.5) | NFE Backward 81(72.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 2400 | Test Loss 0.318408 | NFE 74 [*]
Iter 2500 | Time 1.0460(1.1000) | Loss 0.369229(0.353485) | NFE Forward 80(81.8) | NFE Backward 69(74.1) | CNF Time 0.5000(0.5000)
[TEST] Iter 2500 | Test Loss 0.347938 | NFE 86 []
Iter 2600 | Time 1.2631(1.1353) | Loss 0.378309(0.369365) | NFE Forward 92(85.5) | NFE Backward 87(76.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 2600 | Test Loss 0.341991 | NFE 86 []
Iter 2700 | Time 1.2873(1.1489) | Loss 0.452595(0.355739) | NFE Forward 98(82.9) | NFE Backward 87(79.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 2700 | Test Loss 0.297260 | NFE 74 [*]
Iter 2800 | Time 1.1353(1.2127) | Loss 0.284135(0.345332) | NFE Forward 74(89.5) | NFE Backward 81(82.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 2800 | Test Loss 0.246111 | NFE 92 [*]
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=100, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, gpu=0, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1000, test_rtol=None, test_solver=None, time_length=0.5, train_T=True, val_freq=100, viz_freq=100, weight_decay=1e-05)
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=2, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=64, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=64, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=64, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=64, out_features=2, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=2, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=2, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 9225
Iter 2900 | Time 1.1841(1.2392) | Loss 0.391748(0.359735) | NFE Forward 86(91.2) | NFE Backward 81(84.5) | CNF Time 0.5000(0.5000)
[TEST] Iter 2900 | Test Loss 0.328515 | NFE 86 []
Iter 0000 | Time 13.3330(13.3330) | Loss 2.472296(2.472296) | NFE Forward 14(14.0) | NFE Backward 15(15.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 0000 | Test Loss 2.468206 | NFE 14 [*]
Iter 0100 | Time 0.5793(0.5768) | Loss 1.953905(1.963094) | NFE Forward 20(20.0) | NFE Backward 27(24.0) | CNF Time 0.3718(0.3875)
[TEST] Iter 0100 | Test Loss 1.938514 | NFE 20 [*]
Iter 3000 | Time 1.1841(1.2475) | Loss 0.309775(0.343123) | NFE Forward 86(91.0) | NFE Backward 81(85.0) | CNF Time 0.5000(0.5000)
[TEST] Iter 3000 | Test Loss 0.286439 | NFE 86 []
711232) | NFE Forward 32(27.5) | NFE Backward 27(28.0) | CNF Time 0.2700(0.2823)
[TEST] Iter 0200 | Test Loss 1.678164 | NFE 32 [*]
Iter 0300 | Time 0.8276(0.8286) | Loss 1.647888(1.658862) | NFE Forward 32(33.3) | NFE Backward 33(32.2) | CNF Time 0.1911(0.2006)
[TEST] Iter 0300 | Test Loss 1.659026 | NFE 32 [*]
Iter 3100 | Time 1.3514(1.2584) | Loss 0.334107(0.339101) | NFE Forward 110(90.4) | NFE Backward 87(86.6) | CNF Time 0.5000(0.5000)
[TEST] Iter 3100 | Test Loss 0.340733 | NFE 86 []
Iter 0400 | Time 0.8470(0.7451) | Loss 1.557732(1.546442) | NFE Forward 26(29.5) | NFE Backward 27(27.8) | CNF Time 0.1315(0.1386)
[TEST] Iter 0400 | Test Loss 1.516204 | NFE 26 [*]
Iter 0500 | Time 0.9707(0.9004) | Loss 1.462154(1.461540) | NFE Forward 32(34.0) | NFE Backward 45(37.1) | CNF Time 0.0877(0.0929)
[TEST] Iter 0500 | Test Loss 1.464257 | NFE 32 [*]
Iter 3200 | Time 1.2994(1.3073) | Loss 0.349894(0.336291) | NFE Forward 98(95.0) | NFE Backward 87(89.4) | CNF Time 0.5000(0.5000)
[TEST] Iter 3200 | Test Loss 0.328823 | NFE 92 []
Iter 0600 | Time 0.9645(0.9382) | Loss 1.427234(1.424265) | NFE Forward 32(32.9) | NFE Backward 51(40.8) | CNF Time 0.0566(0.0602)
[TEST] Iter 0600 | Test Loss 1.435738 | NFE 32 [*]
Iter 0700 | Time 0.8156(0.8626) | Loss 1.489052(1.465974) | NFE Forward 32(32.1) | NFE Backward 33(36.3) | CNF Time 0.0351(0.0376)
[TEST] Iter 0700 | Test Loss 1.480816 | NFE 32 []
Iter 0800 | Time 0.8550(0.8637) | Loss 1.633553(1.597260) | NFE Forward 32(34.0) | NFE Backward 39(36.6) | CNF Time 0.0210(0.0226)
[TEST] Iter 0800 | Test Loss 1.619815 | NFE 32 []
Iter 0900 | Time 0.7294(0.7438) | Loss 1.794463(1.767338) | NFE Forward 26(27.3) | NFE Backward 33(32.1) | CNF Time 0.0120(0.0130)
[TEST] Iter 0900 | Test Loss 1.780951 | NFE 26 []
Iter 1000 | Time 0.6834(0.7092) | Loss 1.967809(1.958237) | NFE Forward 26(24.6) | NFE Backward 33(32.1) | CNF Time 0.0066(0.0072)
[TEST] Iter 1000 | Test Loss 1.971731 | NFE 32 []
Iter 1100 | Time 0.6471(0.6330) | Loss 2.177160(2.144080) | NFE Forward 26(24.4) | NFE Backward 27(27.8) | CNF Time 0.0034(0.0038)
[TEST] Iter 1100 | Test Loss 2.167290 | NFE 26 []
Iter 1200 | Time 0.4660(0.4734) | Loss 2.299521(2.291272) | NFE Forward 20(19.6) | NFE Backward 21(21.2) | CNF Time 0.0017(0.0019)
[TEST] Iter 1200 | Test Loss 2.307953 | NFE 20 []
Iter 1300 | Time 0.4640(0.4840) | Loss 2.393080(2.386598) | NFE Forward 20(19.6) | NFE Backward 21(21.4) | CNF Time 0.0008(0.0009)
[TEST] Iter 1300 | Test Loss 2.399638 | NFE 20 []
Iter 1400 | Time 0.5234(0.5036) | Loss 2.444135(2.443631) | NFE Forward 20(19.8) | NFE Backward 27(22.6) | CNF Time 0.0004(0.0004)
[TEST] Iter 1400 | Test Loss 2.448762 | NFE 20 []
Iter 1500 | Time 0.4788(0.5122) | Loss 2.475831(2.472499) | NFE Forward 20(20.0) | NFE Backward 21(22.2) | CNF Time 0.0001(0.0002)
[TEST] Iter 1500 | Test Loss 2.482075 | NFE 20 []
Iter 1600 | Time 0.5144(0.5138) | Loss 2.483870(2.486377) | NFE Forward 20(20.0) | NFE Backward 21(23.1) | CNF Time 0.0001(0.0001)
[TEST] Iter 1600 | Test Loss 2.483017 | NFE 20 []
Iter 1700 | Time 0.5290(0.5150) | Loss 2.494584(2.491416) | NFE Forward 20(20.0) | NFE Backward 21(22.2) | CNF Time 0.0000(0.0000)
[TEST] Iter 1700 | Test Loss 2.492851 | NFE 20 []
Iter 1800 | Time 0.4613(0.5091) | Loss 2.492468(2.494266) | NFE Forward 14(19.5) | NFE Backward 21(22.3) | CNF Time 0.0000(0.0000)
[TEST] Iter 1800 | Test Loss 2.496845 | NFE 20 []
Iter 1900 | Time 0.5275(0.5082) | Loss 2.498943(2.494890) | NFE Forward 20(19.7) | NFE Backward 21(22.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 1900 | Test Loss 2.493134 | NFE 20 []
Iter 2000 | Time 0.5119(0.5049) | Loss 2.496050(2.494190) | NFE Forward 20(20.0) | NFE Backward 21(21.8) | CNF Time 0.0000(0.0000)
[TEST] Iter 2000 | Test Loss 2.495107 | NFE 20 []
Iter 2100 | Time 0.5178(0.5125) | Loss 2.494964(2.495183) | NFE Forward 20(19.8) | NFE Backward 27(22.8) | CNF Time 0.0000(0.0000)
[TEST] Iter 2100 | Test Loss 2.493013 | NFE 20 []
Iter 2200 | Time 0.5266(0.5145) | Loss 2.502920(2.495570) | NFE Forward 20(19.9) | NFE Backward 27(22.8) | CNF Time 0.0000(0.0000)
[TEST] Iter 2200 | Test Loss 2.496040 | NFE 20 []
Iter 2300 | Time 0.4693(0.5054) | Loss 2.496576(2.495017) | NFE Forward 20(20.0) | NFE Backward 21(22.4) | CNF Time 0.0000(0.0000)
[TEST] Iter 2300 | Test Loss 2.498814 | NFE 20 []
Iter 2400 | Time 0.5113(0.4964) | Loss 2.493896(2.495772) | NFE Forward 20(19.6) | NFE Backward 27(21.9) | CNF Time 0.0000(0.0000)
[TEST] Iter 2400 | Test Loss 2.495577 | NFE 20 []
Iter 2500 | Time 0.5187(0.4903) | Loss 2.493290(2.496126) | NFE Forward 20(19.8) | NFE Backward 21(21.3) | CNF Time 0.0000(0.0000)
[TEST] Iter 2500 | Test Loss 2.492198 | NFE 20 []
Iter 2600 | Time 0.4759(0.4908) | Loss 2.500962(2.494525) | NFE Forward 20(19.8) | NFE Backward 21(21.3) | CNF Time 0.0000(0.0000)
[TEST] Iter 2600 | Test Loss 2.499933 | NFE 20 []
Iter 2700 | Time 0.5138(0.4936) | Loss 2.495091(2.494939) | NFE Forward 20(19.6) | NFE Backward 27(21.9) | CNF Time 0.0000(0.0000)
[TEST] Iter 2700 | Test Loss 2.496485 | NFE 20 []
Iter 2800 | Time 0.4549(0.4780) | Loss 2.493398(2.495473) | NFE Forward 20(19.9) | NFE Backward 21(21.1) | CNF Time 0.0000(0.0000)
[TEST] Iter 2800 | Test Loss 2.495195 | NFE 20 []
Iter 2900 | Time 0.4865(0.4846) | Loss 2.494968(2.494226) | NFE Forward 20(20.0) | NFE Backward 21(21.2) | CNF Time 0.0000(0.0000)
[TEST] Iter 2900 | Test Loss 2.499285 | NFE 20 []
Iter 3000 | Time 0.4720(0.4830) | Loss 2.492375(2.495747) | NFE Forward 20(19.5) | NFE Backward 21(21.4) | CNF Time 0.0000(0.0000)
[TEST] Iter 3000 | Test Loss 2.494009 | NFE 20 []
Iter 3100 | Time 0.4770(0.4765) | Loss 2.492125(2.494765) | NFE Forward 20(19.9) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3100 | Test Loss 2.496904 | NFE 20 []
Iter 3200 | Time 0.4722(0.4719) | Loss 2.498013(2.495791) | NFE Forward 20(19.9) | NFE Backward 21(21.1) | CNF Time 0.0000(0.0000)
[TEST] Iter 3200 | Test Loss 2.500115 | NFE 20 []
Iter 3300 | Time 0.4645(0.4762) | Loss 2.493996(2.495684) | NFE Forward 20(19.9) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3300 | Test Loss 2.493217 | NFE 20 []
Iter 3400 | Time 0.4601(0.4729) | Loss 2.493412(2.494732) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3400 | Test Loss 2.498284 | NFE 20 []
Iter 3500 | Time 0.4738(0.4723) | Loss 2.499558(2.495707) | NFE Forward 20(19.8) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3500 | Test Loss 2.490129 | NFE 20 []
Iter 3600 | Time 0.4654(0.4713) | Loss 2.496874(2.494065) | NFE Forward 20(19.7) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3600 | Test Loss 2.487099 | NFE 20 []
Iter 3700 | Time 0.5205(0.4725) | Loss 2.493303(2.495602) | NFE Forward 20(19.7) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3700 | Test Loss 2.490753 | NFE 20 []
Iter 3800 | Time 0.4793(0.4742) | Loss 2.495382(2.496024) | NFE Forward 20(19.8) | NFE Backward 21(21.3) | CNF Time 0.0000(0.0000)
[TEST] Iter 3800 | Test Loss 2.500959 | NFE 20 []
Iter 3900 | Time 0.4653(0.4686) | Loss 2.499078(2.495543) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 3900 | Test Loss 2.507476 | NFE 20 []
Iter 4000 | Time 0.4778(0.4733) | Loss 2.491665(2.494958) | NFE Forward 20(19.4) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 4000 | Test Loss 2.500544 | NFE 20 []
Iter 4100 | Time 0.4626(0.4673) | Loss 2.498121(2.494442) | NFE Forward 20(19.3) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 4100 | Test Loss 2.488545 | NFE 20 []
Iter 4200 | Time 0.4795(0.4708) | Loss 2.502789(2.494762) | NFE Forward 20(19.9) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 4200 | Test Loss 2.498373 | NFE 20 []
Iter 4300 | Time 0.4603(0.4680) | Loss 2.495788(2.495353) | NFE Forward 20(19.5) | NFE Backward 21(21.1) | CNF Time 0.0000(0.0000)
[TEST] Iter 4300 | Test Loss 2.497100 | NFE 20 []
Iter 4400 | Time 0.4758(0.4687) | Loss 2.498219(2.494946) | NFE Forward 14(19.2) | NFE Backward 21(21.1) | CNF Time 0.0000(0.0000)
[TEST] Iter 4400 | Test Loss 2.487953 | NFE 14 []
Iter 4500 | Time 0.4652(0.4685) | Loss 2.500176(2.496339) | NFE Forward 20(18.9) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 4500 | Test Loss 2.494857 | NFE 20 []
Iter 4600 | Time 0.4590(0.4667) | Loss 2.498913(2.494632) | NFE Forward 14(18.1) | NFE Backward 21(21.0) | CNF Time 0.0000(0.0000)
[TEST] Iter 4600 | Test Loss 2.495548 | NFE 20 []
Iter 4700 | Time 0.4574(0.4732) | Loss 2.498049(2.494440) | NFE Forward 20(18.8) | NFE Backward 21(21.1) | CNF Time 0.0000(0.0000)
[TEST] Iter 4700 | Test Loss 2.493306 | NFE 20 []
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, data='pinwheel', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, nhidden=1, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='hep', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, nhidden=1, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='hep', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, nhidden=1, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, nhidden=1, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, nhidden=1, niters=10000, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 00 | Iter 0000 | Time 3.2759(3.2759) | Loss 37.758087(37.758087) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.0000(1.0000)
[VAL] Epoch 00 | Val Loss 37.188291 | NFE 20 | NoImproveEpochs 01/16
Epoch 00 | Iter 0010 | Time 2.5974(3.2231) | Loss 35.672089(37.201101) | NFE Forward 20(20.0) | NFE Backward 27(22.5) | CNF Time 1.0000(1.0000)
Epoch 00 | Iter 0020 | Time 2.8299(3.0710) | Loss 34.791130(36.516213) | NFE Forward 26(21.5) | NFE Backward 27(24.0) | CNF Time 1.0000(1.0000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 00 | Iter 0000 | Time 3.084(3.084) | Loss 37.604(37.604) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss 37.147 | NFE 20 | NoImproveEpochs 01/16
Epoch 00 | Iter 0010 | Time 2.646(3.126) | Loss 35.558(37.078) | NFE Forward 20(20.0) | NFE Backward 27(22.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0020 | Time 3.307(3.045) | Loss 34.832(36.430) | NFE Forward 26(21.7) | NFE Backward 33(24.4) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 00 | Iter 0000 | Time 3.701(3.701) | Loss 37.248(37.248) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss 36.849 | NFE 20 | NoImproveEpochs 01/16
Epoch 00 | Iter 0010 | Time 2.630(3.518) | Loss 35.556(36.819) | NFE Forward 20(20.0) | NFE Backward 27(22.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0020 | Time 2.862(3.260) | Loss 34.724(36.233) | NFE Forward 26(21.1) | NFE Backward 27(24.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0030 | Time 2.850(3.138) | Loss 32.284(35.300) | NFE Forward 26(22.7) | NFE Backward 27(25.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0040 | Time 3.115(3.043) | Loss 26.670(33.274) | NFE Forward 32(24.1) | NFE Backward 27(25.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0050 | Time 4.747(3.397) | Loss 20.107(29.741) | NFE Forward 32(26.7) | NFE Backward 51(30.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0060 | Time 4.369(3.801) | Loss 14.261(25.383) | NFE Forward 32(28.5) | NFE Backward 45(36.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0070 | Time 4.974(4.181) | Loss 8.804(20.597) | NFE Forward 38(31.9) | NFE Backward 51(41.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0080 | Time 5.434(4.531) | Loss 3.744(15.668) | NFE Forward 38(33.9) | NFE Backward 57(45.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0090 | Time 5.795(4.901) | Loss 0.243(10.889) | NFE Forward 50(37.7) | NFE Backward 57(49.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0100 | Time 5.905(5.206) | Loss -2.300(6.630) | NFE Forward 50(41.8) | NFE Backward 57(51.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0110 | Time 5.810(5.412) | Loss -5.423(2.896) | NFE Forward 50(44.6) | NFE Backward 57(53.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0120 | Time 5.818(5.557) | Loss -8.032(-0.313) | NFE Forward 50(46.4) | NFE Backward 57(54.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0130 | Time 6.267(5.745) | Loss -10.240(-3.226) | NFE Forward 50(47.6) | NFE Backward 63(57.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0140 | Time 6.286(5.887) | Loss -12.078(-5.775) | NFE Forward 50(48.4) | NFE Backward 63(58.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0150 | Time 6.280(6.009) | Loss -13.574(-8.101) | NFE Forward 50(48.9) | NFE Backward 63(60.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0160 | Time 6.299(6.099) | Loss -14.400(-9.973) | NFE Forward 50(49.3) | NFE Backward 63(61.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0170 | Time 6.273(6.148) | Loss -16.007(-11.757) | NFE Forward 50(49.5) | NFE Backward 63(61.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0180 | Time 6.284(6.193) | Loss -17.085(-13.259) | NFE Forward 50(49.7) | NFE Backward 63(62.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0190 | Time 6.223(6.210) | Loss -18.526(-14.897) | NFE Forward 50(49.8) | NFE Backward 63(62.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0200 | Time 6.236(6.237) | Loss -19.509(-16.340) | NFE Forward 50(50.3) | NFE Backward 63(62.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0210 | Time 6.156(6.265) | Loss -18.951(-17.723) | NFE Forward 50(50.6) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0220 | Time 6.228(6.270) | Loss -21.716(-18.809) | NFE Forward 50(50.8) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0230 | Time 6.242(6.260) | Loss -23.859(-20.033) | NFE Forward 50(50.5) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0240 | Time 6.212(6.271) | Loss -24.197(-21.260) | NFE Forward 50(51.1) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0250 | Time 6.184(6.384) | Loss -23.783(-21.873) | NFE Forward 50(52.7) | NFE Backward 63(63.9) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -23.722 | NFE 50 | NoImproveEpochs 02/16
Epoch 00 | Iter 0260 | Time 6.208(6.991) | Loss -25.176(-22.670) | NFE Forward 50(52.1) | NFE Backward 63(63.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0270 | Time 6.537(6.762) | Loss -24.883(-23.588) | NFE Forward 62(52.6) | NFE Backward 63(63.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0280 | Time 7.388(6.743) | Loss -24.936(-24.294) | NFE Forward 62(54.5) | NFE Backward 75(64.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0290 | Time 6.636(6.701) | Loss -26.334(-25.013) | NFE Forward 62(55.8) | NFE Backward 63(64.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0300 | Time 6.435(6.590) | Loss -28.351(-25.824) | NFE Forward 56(55.7) | NFE Backward 63(64.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0310 | Time 7.015(6.656) | Loss -27.936(-26.575) | NFE Forward 62(56.2) | NFE Backward 69(65.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0320 | Time 7.454(6.727) | Loss -27.315(-27.097) | NFE Forward 62(57.2) | NFE Backward 75(66.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0330 | Time 6.811(6.723) | Loss -30.145(-27.839) | NFE Forward 56(57.3) | NFE Backward 69(66.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0340 | Time 6.825(6.746) | Loss -30.561(-28.609) | NFE Forward 56(57.1) | NFE Backward 69(66.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0350 | Time 7.056(6.812) | Loss -30.233(-29.128) | NFE Forward 62(57.6) | NFE Backward 69(67.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0360 | Time 7.067(6.829) | Loss -30.475(-29.628) | NFE Forward 62(57.9) | NFE Backward 69(67.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0370 | Time 6.766(6.816) | Loss -31.957(-30.240) | NFE Forward 56(57.2) | NFE Backward 69(68.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0380 | Time 6.849(6.820) | Loss -31.420(-30.676) | NFE Forward 56(56.8) | NFE Backward 69(68.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0390 | Time 6.827(6.854) | Loss -32.143(-30.993) | NFE Forward 56(57.2) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0400 | Time 6.866(6.838) | Loss -32.743(-31.494) | NFE Forward 56(56.8) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0410 | Time 6.688(6.828) | Loss -33.117(-31.930) | NFE Forward 56(56.5) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0420 | Time 6.881(6.829) | Loss -33.091(-32.275) | NFE Forward 56(56.3) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0430 | Time 7.184(6.869) | Loss -32.048(-32.450) | NFE Forward 56(56.2) | NFE Backward 75(69.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0440 | Time 6.696(6.847) | Loss -33.126(-32.642) | NFE Forward 56(56.2) | NFE Backward 69(69.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0450 | Time 6.849(6.843) | Loss -33.879(-33.029) | NFE Forward 56(56.1) | NFE Backward 69(69.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0460 | Time 6.823(6.865) | Loss -33.687(-33.280) | NFE Forward 56(56.1) | NFE Backward 69(69.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0470 | Time 6.835(6.859) | Loss -33.410(-33.493) | NFE Forward 56(56.0) | NFE Backward 69(69.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0480 | Time 6.961(6.865) | Loss -34.537(-33.695) | NFE Forward 56(56.0) | NFE Backward 69(69.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0490 | Time 6.841(6.851) | Loss -33.715(-33.900) | NFE Forward 56(56.0) | NFE Backward 69(69.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0500 | Time 6.679(6.826) | Loss -35.103(-34.092) | NFE Forward 56(56.0) | NFE Backward 69(69.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0510 | Time 6.717(6.830) | Loss -34.696(-34.222) | NFE Forward 56(56.0) | NFE Backward 69(69.3) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -34.059 | NFE 56 | NoImproveEpochs 03/16
Epoch 00 | Iter 0520 | Time 6.862(7.488) | Loss -34.759(-34.336) | NFE Forward 56(56.0) | NFE Backward 69(69.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0530 | Time 6.741(7.284) | Loss -34.726(-34.483) | NFE Forward 56(56.0) | NFE Backward 69(70.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0540 | Time 6.711(7.154) | Loss -34.744(-34.562) | NFE Forward 56(56.0) | NFE Backward 69(70.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0550 | Time 6.879(7.047) | Loss -35.054(-34.911) | NFE Forward 56(56.0) | NFE Backward 69(69.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0560 | Time 6.763(7.014) | Loss -35.720(-35.194) | NFE Forward 56(56.0) | NFE Backward 69(70.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0570 | Time 7.248(7.063) | Loss -34.662(-34.984) | NFE Forward 56(56.0) | NFE Backward 75(71.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0580 | Time 6.853(7.040) | Loss -35.160(-35.112) | NFE Forward 56(56.0) | NFE Backward 69(71.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0590 | Time 6.846(7.011) | Loss -36.218(-35.391) | NFE Forward 56(56.0) | NFE Backward 69(71.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0600 | Time 6.842(6.965) | Loss -36.005(-35.624) | NFE Forward 56(56.0) | NFE Backward 69(70.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0610 | Time 6.852(6.972) | Loss -36.454(-35.833) | NFE Forward 56(56.0) | NFE Backward 69(70.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0620 | Time 7.275(6.994) | Loss -35.952(-35.953) | NFE Forward 56(56.0) | NFE Backward 75(71.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0630 | Time 7.187(7.029) | Loss -36.548(-36.109) | NFE Forward 56(56.0) | NFE Backward 75(71.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0640 | Time 7.251(6.997) | Loss -35.861(-36.379) | NFE Forward 56(56.0) | NFE Backward 75(71.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0650 | Time 7.228(7.002) | Loss -35.289(-36.442) | NFE Forward 56(56.0) | NFE Backward 75(71.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0660 | Time 7.124(7.069) | Loss -36.136(-36.285) | NFE Forward 56(56.0) | NFE Backward 75(72.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0670 | Time 7.127(7.083) | Loss -36.952(-36.308) | NFE Forward 56(56.0) | NFE Backward 75(73.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0680 | Time 7.284(7.117) | Loss -37.379(-36.526) | NFE Forward 56(56.0) | NFE Backward 75(73.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0690 | Time 6.871(7.101) | Loss -37.204(-36.720) | NFE Forward 56(56.0) | NFE Backward 69(73.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0700 | Time 7.210(7.051) | Loss -35.527(-36.873) | NFE Forward 56(56.0) | NFE Backward 75(72.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0710 | Time 7.178(7.116) | Loss -35.640(-36.615) | NFE Forward 56(56.2) | NFE Backward 75(73.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0720 | Time 7.245(7.145) | Loss -37.448(-36.839) | NFE Forward 56(56.1) | NFE Backward 75(73.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0730 | Time 6.766(7.061) | Loss -37.315(-37.180) | NFE Forward 56(56.1) | NFE Backward 69(72.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0740 | Time 7.219(7.058) | Loss -38.507(-37.400) | NFE Forward 56(56.3) | NFE Backward 75(72.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0750 | Time 7.205(7.084) | Loss -38.183(-37.524) | NFE Forward 56(56.4) | NFE Backward 75(72.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0760 | Time 7.237(7.145) | Loss -38.213(-37.510) | NFE Forward 56(57.1) | NFE Backward 75(73.3) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -38.095 | NFE 56 | NoImproveEpochs 04/16
Epoch 00 | Iter 0770 | Time 6.857(7.945) | Loss -37.958(-37.596) | NFE Forward 56(57.5) | NFE Backward 69(73.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0780 | Time 7.198(7.711) | Loss -37.750(-37.559) | NFE Forward 56(57.8) | NFE Backward 75(73.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0790 | Time 7.336(7.551) | Loss -38.133(-37.742) | NFE Forward 62(57.6) | NFE Backward 75(74.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0800 | Time 7.449(7.415) | Loss -38.843(-38.006) | NFE Forward 62(57.5) | NFE Backward 75(73.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0810 | Time 7.215(7.379) | Loss -39.231(-38.152) | NFE Forward 56(58.0) | NFE Backward 75(74.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0820 | Time 7.332(7.378) | Loss -37.943(-38.148) | NFE Forward 62(58.9) | NFE Backward 75(74.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0830 | Time 7.433(7.391) | Loss -37.945(-38.219) | NFE Forward 62(59.8) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0840 | Time 7.846(7.420) | Loss -38.917(-38.435) | NFE Forward 62(60.5) | NFE Backward 81(75.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0850 | Time 7.439(7.384) | Loss -38.516(-38.520) | NFE Forward 62(60.4) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0860 | Time 7.397(7.392) | Loss -38.112(-38.349) | NFE Forward 62(61.0) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0870 | Time 7.436(7.400) | Loss -39.208(-38.480) | NFE Forward 62(61.1) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0880 | Time 7.323(7.385) | Loss -39.551(-38.683) | NFE Forward 62(61.2) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0890 | Time 7.420(7.366) | Loss -39.581(-38.880) | NFE Forward 62(60.9) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0900 | Time 7.461(7.389) | Loss -38.531(-38.904) | NFE Forward 62(61.2) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0910 | Time 7.428(7.396) | Loss -38.594(-38.818) | NFE Forward 62(61.3) | NFE Backward 75(75.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0920 | Time 7.438(7.423) | Loss -39.642(-38.732) | NFE Forward 62(61.5) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0930 | Time 7.393(7.427) | Loss -38.930(-38.828) | NFE Forward 62(61.7) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0940 | Time 7.822(7.473) | Loss -39.643(-39.192) | NFE Forward 62(61.6) | NFE Backward 81(75.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0950 | Time 7.424(7.471) | Loss -39.175(-39.242) | NFE Forward 62(61.7) | NFE Backward 75(75.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0960 | Time 7.901(7.467) | Loss -39.325(-39.517) | NFE Forward 62(61.6) | NFE Backward 81(75.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0970 | Time 7.429(7.500) | Loss -38.306(-39.351) | NFE Forward 62(61.8) | NFE Backward 75(76.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0980 | Time 7.377(7.490) | Loss -40.501(-39.364) | NFE Forward 62(61.8) | NFE Backward 75(75.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 0990 | Time 7.387(7.484) | Loss -39.177(-39.464) | NFE Forward 62(61.9) | NFE Backward 75(75.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1000 | Time 7.456(7.467) | Loss -40.765(-39.579) | NFE Forward 62(61.9) | NFE Backward 75(75.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1010 | Time 7.773(7.495) | Loss -40.841(-39.800) | NFE Forward 62(62.0) | NFE Backward 81(76.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1020 | Time 7.884(7.547) | Loss -39.696(-39.770) | NFE Forward 62(62.0) | NFE Backward 81(76.7) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -39.813 | NFE 62 | NoImproveEpochs 05/16
Epoch 00 | Iter 1030 | Time 7.874(8.328) | Loss -40.243(-39.780) | NFE Forward 62(62.0) | NFE Backward 81(77.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1040 | Time 7.832(8.059) | Loss -40.266(-39.887) | NFE Forward 62(62.0) | NFE Backward 81(77.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1050 | Time 7.423(7.879) | Loss -40.135(-39.957) | NFE Forward 62(62.0) | NFE Backward 75(76.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1060 | Time 7.859(7.823) | Loss -40.282(-40.035) | NFE Forward 62(62.0) | NFE Backward 81(77.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1070 | Time 7.036(7.752) | Loss -41.090(-40.124) | NFE Forward 62(62.0) | NFE Backward 69(77.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1080 | Time 7.358(7.723) | Loss -40.247(-40.248) | NFE Forward 62(62.0) | NFE Backward 75(77.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1090 | Time 7.891(7.695) | Loss -40.756(-40.480) | NFE Forward 62(62.0) | NFE Backward 81(77.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1100 | Time 7.817(7.722) | Loss -40.055(-40.539) | NFE Forward 62(62.0) | NFE Backward 81(78.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1110 | Time 7.868(7.736) | Loss -40.666(-40.596) | NFE Forward 62(62.0) | NFE Backward 81(79.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1120 | Time 7.843(7.728) | Loss -40.735(-40.633) | NFE Forward 62(62.0) | NFE Backward 81(79.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1130 | Time 7.824(7.742) | Loss -41.216(-40.646) | NFE Forward 62(62.0) | NFE Backward 81(79.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1140 | Time 7.839(7.711) | Loss -40.494(-40.677) | NFE Forward 62(62.0) | NFE Backward 81(78.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1150 | Time 7.436(7.730) | Loss -40.563(-40.783) | NFE Forward 62(62.0) | NFE Backward 75(79.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1160 | Time 7.892(7.750) | Loss -41.158(-40.793) | NFE Forward 62(62.0) | NFE Backward 81(79.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1170 | Time 7.827(7.754) | Loss -41.262(-41.080) | NFE Forward 62(62.0) | NFE Backward 81(79.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1180 | Time 7.900(7.779) | Loss -41.931(-41.133) | NFE Forward 62(62.0) | NFE Backward 81(80.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1190 | Time 7.802(7.752) | Loss -41.304(-41.063) | NFE Forward 62(62.0) | NFE Backward 81(79.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1200 | Time 7.858(7.788) | Loss -40.665(-41.137) | NFE Forward 62(62.0) | NFE Backward 81(80.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1210 | Time 7.924(7.813) | Loss -41.763(-41.254) | NFE Forward 62(62.0) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1220 | Time 7.890(7.785) | Loss -41.823(-41.305) | NFE Forward 62(62.0) | NFE Backward 81(80.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1230 | Time 7.855(7.780) | Loss -41.739(-41.289) | NFE Forward 62(62.0) | NFE Backward 81(80.1) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1240 | Time 7.916(7.807) | Loss -42.477(-41.438) | NFE Forward 62(62.0) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1250 | Time 7.471(7.779) | Loss -41.084(-41.617) | NFE Forward 62(62.0) | NFE Backward 75(80.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1260 | Time 7.850(7.799) | Loss -42.862(-41.753) | NFE Forward 62(62.0) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1270 | Time 7.864(7.799) | Loss -40.073(-41.677) | NFE Forward 62(62.0) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1280 | Time 7.484(7.778) | Loss -42.500(-41.789) | NFE Forward 62(62.0) | NFE Backward 75(80.1) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -41.529 | NFE 62 | NoImproveEpochs 06/16
Epoch 00 | Iter 1290 | Time 7.479(8.414) | Loss -42.332(-41.834) | NFE Forward 62(62.0) | NFE Backward 75(80.2) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1300 | Time 7.855(8.230) | Loss -42.480(-41.827) | NFE Forward 62(62.0) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1310 | Time 7.918(8.113) | Loss -42.510(-42.045) | NFE Forward 62(62.0) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1320 | Time 7.872(8.011) | Loss -42.542(-42.213) | NFE Forward 62(62.0) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1330 | Time 7.917(7.939) | Loss -42.114(-42.060) | NFE Forward 62(62.0) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1340 | Time 7.870(7.884) | Loss -43.244(-42.206) | NFE Forward 62(62.0) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1350 | Time 7.822(7.850) | Loss -42.696(-42.401) | NFE Forward 62(62.0) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1360 | Time 7.917(7.841) | Loss -41.582(-42.629) | NFE Forward 62(62.0) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1370 | Time 7.929(7.835) | Loss -43.018(-42.608) | NFE Forward 62(62.0) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1380 | Time 7.844(7.824) | Loss -42.704(-42.661) | NFE Forward 62(62.0) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1390 | Time 7.830(7.826) | Loss -43.326(-42.843) | NFE Forward 62(62.0) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1400 | Time 7.852(7.816) | Loss -42.419(-42.704) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1410 | Time 7.758(7.825) | Loss -41.978(-42.645) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1420 | Time 7.835(7.826) | Loss -42.986(-42.761) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1430 | Time 7.908(7.845) | Loss -42.929(-42.901) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1440 | Time 7.756(7.857) | Loss -43.431(-42.926) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1450 | Time 7.790(7.861) | Loss -43.672(-43.059) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1460 | Time 7.923(7.869) | Loss -42.404(-42.971) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1470 | Time 7.905(7.878) | Loss -43.998(-42.981) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1480 | Time 7.857(7.872) | Loss -43.225(-43.150) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1490 | Time 7.854(7.851) | Loss -42.726(-43.109) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1500 | Time 7.926(7.855) | Loss -43.870(-43.034) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1510 | Time 7.912(7.840) | Loss -44.201(-43.148) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1520 | Time 7.933(7.859) | Loss -43.188(-43.213) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1530 | Time 7.742(7.853) | Loss -43.952(-43.454) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -43.629 | NFE 62 | NoImproveEpochs 07/16
Epoch 00 | Iter 1540 | Time 7.754(8.655) | Loss -42.436(-43.474) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1550 | Time 7.894(8.370) | Loss -44.168(-43.585) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1560 | Time 7.922(8.174) | Loss -43.792(-43.632) | NFE Forward 62(62.0) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1570 | Time 7.896(8.069) | Loss -44.330(-43.635) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1580 | Time 7.783(7.999) | Loss -44.679(-43.828) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1590 | Time 7.777(7.954) | Loss -42.570(-43.908) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1600 | Time 7.761(7.916) | Loss -43.760(-43.903) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1610 | Time 7.873(7.894) | Loss -44.140(-43.814) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1620 | Time 7.770(7.876) | Loss -44.350(-43.840) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1630 | Time 7.891(7.875) | Loss -43.684(-43.967) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1640 | Time 7.933(7.867) | Loss -43.747(-44.118) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1650 | Time 7.906(7.857) | Loss -44.670(-44.157) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1660 | Time 7.896(7.850) | Loss -43.799(-44.233) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1670 | Time 7.887(7.862) | Loss -44.515(-44.381) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1680 | Time 7.713(7.848) | Loss -44.524(-44.440) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1690 | Time 7.949(7.842) | Loss -45.310(-44.473) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1700 | Time 7.882(7.838) | Loss -44.260(-44.661) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1710 | Time 7.886(7.835) | Loss -45.134(-44.756) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1720 | Time 7.862(7.840) | Loss -44.088(-44.560) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1730 | Time 7.838(7.839) | Loss -44.523(-44.441) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1740 | Time 7.883(7.844) | Loss -44.379(-44.492) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1750 | Time 7.913(7.857) | Loss -44.737(-44.569) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1760 | Time 7.770(7.853) | Loss -44.537(-44.700) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1770 | Time 7.926(7.861) | Loss -44.806(-44.730) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1780 | Time 7.753(7.856) | Loss -44.824(-44.674) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1790 | Time 7.841(7.851) | Loss -46.007(-44.930) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -45.474 | NFE 62 | NoImproveEpochs 08/16
Epoch 00 | Iter 1800 | Time 7.704(8.528) | Loss -45.975(-45.182) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1810 | Time 7.880(8.287) | Loss -45.930(-45.425) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1820 | Time 7.776(8.130) | Loss -44.962(-45.317) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1830 | Time 7.816(8.031) | Loss -45.593(-45.328) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1840 | Time 7.875(7.973) | Loss -45.424(-45.318) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1850 | Time 7.941(7.926) | Loss -44.880(-45.367) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1860 | Time 7.832(7.897) | Loss -45.261(-45.469) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1870 | Time 7.906(7.892) | Loss -45.767(-45.487) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1880 | Time 7.915(7.882) | Loss -46.205(-45.610) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1890 | Time 7.919(7.872) | Loss -44.421(-45.716) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1900 | Time 7.920(7.865) | Loss -45.696(-45.560) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1910 | Time 7.877(7.850) | Loss -45.263(-45.429) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1920 | Time 7.830(7.844) | Loss -47.102(-45.633) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1930 | Time 7.749(7.842) | Loss -45.536(-45.943) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1940 | Time 7.846(7.840) | Loss -46.316(-46.034) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1950 | Time 7.824(7.842) | Loss -46.258(-46.022) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1960 | Time 7.896(7.860) | Loss -46.437(-46.116) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1970 | Time 7.854(7.866) | Loss -47.173(-46.298) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1980 | Time 7.900(7.861) | Loss -46.136(-46.318) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 1990 | Time 7.914(7.860) | Loss -45.486(-46.257) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2000 | Time 7.822(7.854) | Loss -45.891(-46.187) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2010 | Time 7.938(7.852) | Loss -47.195(-46.317) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2020 | Time 7.902(7.865) | Loss -46.091(-46.354) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2030 | Time 7.886(7.857) | Loss -47.320(-46.438) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2040 | Time 7.823(7.860) | Loss -45.998(-46.571) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -45.611 | NFE 62 | NoImproveEpochs 09/16
Epoch 00 | Iter 2050 | Time 7.972(8.726) | Loss -45.718(-46.394) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2060 | Time 7.905(8.439) | Loss -46.776(-46.484) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2070 | Time 7.872(8.243) | Loss -47.774(-46.638) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2080 | Time 7.929(8.120) | Loss -46.033(-46.845) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2090 | Time 7.894(8.038) | Loss -47.580(-46.919) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2100 | Time 7.923(7.986) | Loss -46.448(-46.918) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2110 | Time 7.881(7.945) | Loss -46.323(-46.720) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2120 | Time 7.895(7.918) | Loss -46.109(-46.745) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2130 | Time 7.820(7.901) | Loss -46.700(-46.784) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2140 | Time 7.853(7.885) | Loss -46.403(-46.753) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2150 | Time 7.897(7.871) | Loss -47.696(-47.078) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2160 | Time 7.830(7.863) | Loss -48.184(-47.340) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2170 | Time 7.869(7.857) | Loss -46.876(-47.404) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2180 | Time 7.867(7.857) | Loss -46.950(-47.457) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2190 | Time 7.942(7.863) | Loss -47.137(-47.264) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2200 | Time 7.864(7.869) | Loss -47.189(-47.323) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2210 | Time 7.906(7.877) | Loss -48.451(-47.518) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2220 | Time 7.917(7.873) | Loss -47.011(-47.358) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2230 | Time 7.959(7.875) | Loss -48.135(-47.405) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2240 | Time 7.826(7.865) | Loss -49.265(-47.555) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2250 | Time 7.725(7.863) | Loss -48.459(-47.521) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2260 | Time 7.745(7.853) | Loss -47.452(-47.715) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2270 | Time 7.781(7.848) | Loss -48.730(-47.938) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2280 | Time 7.845(7.846) | Loss -48.095(-48.013) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2290 | Time 7.823(7.840) | Loss -46.277(-47.932) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2300 | Time 7.755(7.841) | Loss -48.040(-47.885) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -46.574 | NFE 62 | NoImproveEpochs 10/16
Epoch 00 | Iter 2310 | Time 7.809(8.589) | Loss -47.439(-47.731) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2320 | Time 7.870(8.345) | Loss -49.087(-47.901) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2330 | Time 7.934(8.192) | Loss -48.306(-48.010) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2340 | Time 7.812(8.085) | Loss -49.154(-48.145) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2350 | Time 7.886(8.018) | Loss -48.785(-48.279) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2360 | Time 7.887(7.963) | Loss -48.155(-48.171) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2370 | Time 7.839(7.923) | Loss -48.667(-48.301) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2380 | Time 7.844(7.906) | Loss -48.899(-48.431) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2390 | Time 7.919(7.894) | Loss -48.423(-48.613) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2400 | Time 7.867(7.889) | Loss -49.171(-48.587) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2410 | Time 7.785(7.878) | Loss -47.908(-48.319) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2420 | Time 7.896(7.879) | Loss -48.405(-48.369) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2430 | Time 7.857(7.877) | Loss -49.249(-48.412) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2440 | Time 7.926(7.879) | Loss -49.520(-48.536) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2450 | Time 7.930(7.882) | Loss -49.009(-48.756) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2460 | Time 7.895(7.889) | Loss -48.852(-48.765) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2470 | Time 7.840(7.878) | Loss -50.284(-48.964) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2480 | Time 7.915(7.875) | Loss -49.328(-49.023) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2490 | Time 7.878(7.861) | Loss -49.251(-49.009) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2500 | Time 7.864(7.848) | Loss -48.596(-48.961) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2510 | Time 7.732(7.842) | Loss -48.935(-48.891) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2520 | Time 7.869(7.836) | Loss -47.740(-49.059) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2530 | Time 7.722(7.826) | Loss -49.245(-49.059) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2540 | Time 7.822(7.833) | Loss -49.817(-48.994) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2550 | Time 7.860(7.847) | Loss -48.981(-49.126) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2560 | Time 7.895(7.849) | Loss -50.088(-49.142) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -48.283 | NFE 62 | NoImproveEpochs 11/16
Epoch 00 | Iter 2570 | Time 7.812(8.481) | Loss -49.752(-49.393) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2580 | Time 7.889(8.270) | Loss -48.924(-49.239) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2590 | Time 7.877(8.121) | Loss -49.810(-49.173) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2600 | Time 7.845(8.025) | Loss -48.715(-49.140) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2610 | Time 7.840(7.955) | Loss -50.038(-49.304) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2620 | Time 7.852(7.919) | Loss -49.829(-49.530) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2630 | Time 7.793(7.883) | Loss -50.737(-49.757) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2640 | Time 7.838(7.862) | Loss -50.323(-49.893) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2650 | Time 7.823(7.852) | Loss -49.758(-50.026) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2660 | Time 7.834(7.846) | Loss -50.778(-49.972) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2670 | Time 7.848(7.844) | Loss -49.237(-49.986) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2680 | Time 7.856(7.842) | Loss -49.439(-49.754) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2690 | Time 7.824(7.831) | Loss -50.623(-49.865) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2700 | Time 7.802(7.826) | Loss -49.265(-49.749) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2710 | Time 7.864(7.833) | Loss -50.085(-49.811) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2720 | Time 7.803(7.835) | Loss -51.011(-49.847) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2730 | Time 7.881(7.828) | Loss -49.869(-49.786) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2740 | Time 7.893(7.826) | Loss -50.536(-49.963) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2750 | Time 7.879(7.826) | Loss -51.170(-50.229) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2760 | Time 7.944(7.833) | Loss -51.552(-50.263) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2770 | Time 7.855(7.832) | Loss -50.280(-50.230) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2780 | Time 7.777(7.835) | Loss -50.182(-50.027) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2790 | Time 7.857(7.836) | Loss -50.854(-50.056) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2800 | Time 7.828(7.827) | Loss -49.843(-50.145) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2810 | Time 7.883(7.837) | Loss -50.351(-50.081) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -50.503 | NFE 62 | NoImproveEpochs 12/16
Epoch 00 | Iter 2820 | Time 7.859(8.629) | Loss -50.800(-50.384) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2830 | Time 7.832(8.366) | Loss -50.281(-50.554) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2840 | Time 7.863(8.193) | Loss -49.482(-50.433) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2850 | Time 7.831(8.060) | Loss -50.404(-50.389) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2860 | Time 7.890(7.982) | Loss -49.416(-50.468) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2870 | Time 7.805(7.929) | Loss -50.780(-50.518) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2880 | Time 7.786(7.900) | Loss -50.603(-50.573) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2890 | Time 7.824(7.879) | Loss -52.063(-50.762) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2900 | Time 7.763(7.864) | Loss -50.839(-50.785) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2910 | Time 7.804(7.856) | Loss -51.275(-50.872) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2920 | Time 7.830(7.852) | Loss -50.774(-50.624) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2930 | Time 7.863(7.838) | Loss -51.300(-50.560) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2940 | Time 7.765(7.840) | Loss -49.581(-50.711) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2950 | Time 7.782(7.838) | Loss -51.643(-50.660) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2960 | Time 7.877(7.836) | Loss -50.837(-50.497) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2970 | Time 7.839(7.847) | Loss -51.075(-50.622) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2980 | Time 7.962(7.847) | Loss -52.568(-50.952) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 2990 | Time 7.797(7.850) | Loss -52.050(-51.118) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3000 | Time 7.928(7.850) | Loss -51.124(-51.047) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3010 | Time 7.889(7.859) | Loss -51.038(-51.156) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3020 | Time 7.787(7.854) | Loss -52.188(-51.178) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3030 | Time 7.806(7.846) | Loss -50.433(-51.190) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3040 | Time 7.868(7.850) | Loss -50.205(-50.856) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3050 | Time 7.870(7.832) | Loss -50.792(-50.833) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3060 | Time 7.854(7.826) | Loss -49.551(-51.010) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3070 | Time 7.873(7.833) | Loss -51.720(-51.275) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 00 | Val Loss -51.177 | NFE 62 | NoImproveEpochs 13/16
Epoch 00 | Iter 3080 | Time 7.830(8.527) | Loss -50.959(-51.223) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3090 | Time 7.787(8.298) | Loss -51.860(-51.170) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3100 | Time 7.828(8.139) | Loss -51.317(-51.357) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3110 | Time 7.863(8.049) | Loss -50.901(-51.328) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3120 | Time 7.833(7.979) | Loss -52.237(-51.508) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3130 | Time 7.832(7.930) | Loss -51.767(-51.315) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3140 | Time 7.867(7.909) | Loss -49.028(-50.996) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3150 | Time 7.868(7.892) | Loss -52.063(-51.005) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3160 | Time 7.860(7.874) | Loss -52.367(-51.425) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 00 | Iter 3170 | Time 7.870(7.855) | Loss -52.098(-51.593) | NFE Forward 62(61.6) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0000 | Time 8.111(7.862) | Loss -51.728(-51.638) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 01 | Val Loss -51.820 | NFE 62 | NoImproveEpochs 14/16
Epoch 01 | Iter 0010 | Time 7.882(8.483) | Loss -50.977(-51.459) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0020 | Time 7.883(8.274) | Loss -52.065(-51.251) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0030 | Time 7.749(8.120) | Loss -52.762(-51.550) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0040 | Time 7.832(8.038) | Loss -50.793(-51.445) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0050 | Time 7.864(7.981) | Loss -52.909(-51.470) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0060 | Time 7.663(7.923) | Loss -50.742(-51.458) | NFE Forward 56(61.5) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0070 | Time 7.612(7.891) | Loss -51.342(-51.661) | NFE Forward 56(61.4) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0080 | Time 7.851(7.878) | Loss -50.599(-51.759) | NFE Forward 62(61.6) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0090 | Time 7.825(7.861) | Loss -52.055(-51.737) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0100 | Time 7.894(7.857) | Loss -51.325(-51.653) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0110 | Time 7.838(7.834) | Loss -51.375(-51.618) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0120 | Time 7.814(7.835) | Loss -52.453(-51.712) | NFE Forward 62(61.6) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0130 | Time 7.789(7.835) | Loss -53.062(-51.818) | NFE Forward 62(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0140 | Time 7.811(7.841) | Loss -51.504(-51.952) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0150 | Time 7.900(7.852) | Loss -50.501(-51.758) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0160 | Time 7.668(7.857) | Loss -51.077(-51.539) | NFE Forward 56(61.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0170 | Time 7.810(7.836) | Loss -52.547(-51.781) | NFE Forward 62(61.2) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0180 | Time 7.860(7.827) | Loss -52.366(-52.087) | NFE Forward 62(60.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0190 | Time 7.740(7.827) | Loss -53.153(-52.194) | NFE Forward 56(60.6) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0200 | Time 7.822(7.811) | Loss -51.352(-52.247) | NFE Forward 62(60.5) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0210 | Time 7.849(7.824) | Loss -53.198(-52.229) | NFE Forward 62(61.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0220 | Time 7.874(7.829) | Loss -52.723(-52.298) | NFE Forward 62(61.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0230 | Time 7.872(7.832) | Loss -51.911(-52.232) | NFE Forward 62(61.2) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0240 | Time 7.945(7.817) | Loss -49.846(-52.068) | NFE Forward 62(61.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0250 | Time 7.910(7.821) | Loss -51.571(-51.888) | NFE Forward 62(61.4) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
[VAL] Epoch 01 | Val Loss -52.202 | NFE 56 | NoImproveEpochs 15/16
Epoch 01 | Iter 0260 | Time 7.579(8.533) | Loss -52.204(-52.002) | NFE Forward 56(60.7) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0270 | Time 7.221(8.250) | Loss -52.960(-52.221) | NFE Forward 56(60.3) | NFE Backward 75(80.7) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0280 | Time 7.904(8.084) | Loss -53.705(-52.510) | NFE Forward 62(59.9) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0290 | Time 7.888(8.004) | Loss -51.220(-52.460) | NFE Forward 62(60.4) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0300 | Time 7.880(7.945) | Loss -50.719(-52.306) | NFE Forward 62(60.9) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0310 | Time 7.501(7.867) | Loss -52.273(-52.300) | NFE Forward 56(60.3) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0320 | Time 7.743(7.828) | Loss -53.544(-52.288) | NFE Forward 62(60.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0330 | Time 7.829(7.798) | Loss -52.478(-52.376) | NFE Forward 62(59.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0340 | Time 7.823(7.791) | Loss -52.731(-52.468) | NFE Forward 62(60.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0350 | Time 7.838(7.781) | Loss -52.157(-52.483) | NFE Forward 62(60.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0360 | Time 7.906(7.783) | Loss -53.158(-52.657) | NFE Forward 62(60.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0370 | Time 7.909(7.786) | Loss -53.516(-52.796) | NFE Forward 62(60.2) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0380 | Time 7.889(7.774) | Loss -53.547(-53.012) | NFE Forward 62(59.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0390 | Time 7.870(7.779) | Loss -53.664(-53.041) | NFE Forward 62(60.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0400 | Time 7.816(7.794) | Loss -51.426(-52.768) | NFE Forward 62(60.4) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0410 | Time 7.674(7.792) | Loss -52.823(-52.704) | NFE Forward 56(60.3) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0420 | Time 7.755(7.777) | Loss -51.774(-52.682) | NFE Forward 62(60.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0430 | Time 7.604(7.748) | Loss -52.201(-52.614) | NFE Forward 56(59.3) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0440 | Time 7.571(7.721) | Loss -53.714(-52.935) | NFE Forward 56(59.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0450 | Time 7.555(7.675) | Loss -52.253(-52.912) | NFE Forward 56(58.2) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0460 | Time 7.632(7.688) | Loss -53.067(-52.784) | NFE Forward 56(58.5) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0470 | Time 7.668(7.693) | Loss -52.516(-52.906) | NFE Forward 56(58.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0480 | Time 7.868(7.723) | Loss -51.783(-52.780) | NFE Forward 62(59.4) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0490 | Time 7.910(7.728) | Loss -53.322(-52.847) | NFE Forward 62(59.6) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0500 | Time 7.633(7.717) | Loss -53.237(-52.860) | NFE Forward 56(59.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0510 | Time 7.522(7.712) | Loss -53.237(-52.919) | NFE Forward 56(58.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 01 | Val Loss -51.868 | NFE 61 | NoImproveEpochs 16/16
Epoch 01 | Iter 0520 | Time 7.901(8.395) | Loss -52.968(-53.099) | NFE Forward 62(58.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0530 | Time 7.880(8.187) | Loss -52.740(-52.973) | NFE Forward 62(59.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0540 | Time 7.633(8.027) | Loss -52.883(-53.030) | NFE Forward 56(58.7) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0550 | Time 7.842(7.923) | Loss -53.468(-53.003) | NFE Forward 62(58.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0560 | Time 7.722(7.836) | Loss -53.629(-53.069) | NFE Forward 56(58.1) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0570 | Time 7.823(7.780) | Loss -53.416(-53.140) | NFE Forward 62(57.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0580 | Time 7.828(7.758) | Loss -52.882(-52.992) | NFE Forward 62(58.2) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0590 | Time 7.909(7.753) | Loss -53.038(-53.020) | NFE Forward 62(58.3) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0600 | Time 7.601(7.724) | Loss -53.873(-52.929) | NFE Forward 56(57.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0610 | Time 7.567(7.699) | Loss -54.335(-53.223) | NFE Forward 56(57.4) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0620 | Time 7.689(7.678) | Loss -53.114(-53.407) | NFE Forward 56(57.4) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0630 | Time 7.675(7.678) | Loss -53.529(-53.390) | NFE Forward 56(57.3) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0640 | Time 7.658(7.683) | Loss -53.463(-53.426) | NFE Forward 56(57.4) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0650 | Time 7.638(7.701) | Loss -53.557(-53.277) | NFE Forward 56(57.7) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0660 | Time 7.554(7.677) | Loss -53.485(-53.369) | NFE Forward 56(57.8) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0670 | Time 7.803(7.686) | Loss -53.348(-53.269) | NFE Forward 62(58.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0680 | Time 7.892(7.691) | Loss -54.114(-53.430) | NFE Forward 62(58.2) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0690 | Time 7.641(7.674) | Loss -54.467(-53.592) | NFE Forward 56(57.6) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0700 | Time 7.657(7.656) | Loss -54.396(-53.784) | NFE Forward 56(57.1) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0710 | Time 7.685(7.671) | Loss -53.908(-53.763) | NFE Forward 56(57.3) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0720 | Time 7.628(7.645) | Loss -53.210(-53.595) | NFE Forward 56(57.1) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0730 | Time 7.583(7.670) | Loss -53.170(-53.543) | NFE Forward 56(57.5) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0740 | Time 7.529(7.671) | Loss -53.077(-53.381) | NFE Forward 56(57.6) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0750 | Time 7.669(7.646) | Loss -55.006(-53.660) | NFE Forward 56(57.3) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 01 | Iter 0760 | Time 7.140(7.618) | Loss -54.435(-53.970) | NFE Forward 56(57.5) | NFE Backward 75(80.2) | CNF Time 1.000(1.000)
[VAL] Epoch 01 | Val Loss -53.316 | NFE 56 | NoImproveEpochs 17/16
Training has finished.
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 5.457(5.457) | Loss 36.816(36.816) | NFE Forward 20(20.0) | NFE Backward 27(27.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss 36.481 | NFE 20 | NoImproveEpochs 01/16
Epoch 0 | Iter 10 | Time 2.669(4.681) | Loss 35.457(36.471) | NFE Forward 20(20.0) | NFE Backward 27(26.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 20 | Time 2.880(4.059) | Loss 34.363(35.937) | NFE Forward 26(21.5) | NFE Backward 27(26.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 30 | Time 2.889(3.667) | Loss 31.376(34.909) | NFE Forward 26(23.0) | NFE Backward 27(26.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 40 | Time 3.111(3.400) | Loss 25.299(32.600) | NFE Forward 32(24.5) | NFE Backward 27(26.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 50 | Time 5.057(3.699) | Loss 18.647(28.779) | NFE Forward 38(27.2) | NFE Backward 51(32.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 60 | Time 4.626(4.027) | Loss 13.493(24.402) | NFE Forward 38(30.2) | NFE Backward 45(36.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 70 | Time 4.626(4.240) | Loss 8.402(19.791) | NFE Forward 38(32.7) | NFE Backward 45(39.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 80 | Time 5.490(4.649) | Loss 3.573(15.082) | NFE Forward 38(35.2) | NFE Backward 57(45.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 90 | Time 5.688(4.999) | Loss -0.692(10.458) | NFE Forward 44(38.0) | NFE Backward 57(49.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 100 | Time 5.828(5.239) | Loss -4.205(6.066) | NFE Forward 50(41.0) | NFE Backward 57(51.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 110 | Time 5.782(5.418) | Loss -7.885(1.863) | NFE Forward 50(43.7) | NFE Backward 57(53.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 120 | Time 5.800(5.580) | Loss -10.243(-1.728) | NFE Forward 50(45.8) | NFE Backward 57(55.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 130 | Time 6.203(5.708) | Loss -11.101(-4.594) | NFE Forward 50(47.2) | NFE Backward 63(56.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 140 | Time 6.249(5.810) | Loss -13.149(-7.242) | NFE Forward 50(48.1) | NFE Backward 63(57.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 150 | Time 6.273(5.951) | Loss -14.205(-9.634) | NFE Forward 50(48.8) | NFE Backward 63(59.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 160 | Time 6.206(6.079) | Loss -15.790(-11.345) | NFE Forward 50(49.2) | NFE Backward 63(61.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 170 | Time 6.265(6.133) | Loss -17.298(-13.032) | NFE Forward 50(49.5) | NFE Backward 63(61.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 180 | Time 6.264(6.171) | Loss -18.660(-14.681) | NFE Forward 50(49.6) | NFE Backward 63(62.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 190 | Time 6.612(6.260) | Loss -17.874(-15.871) | NFE Forward 62(50.9) | NFE Backward 63(62.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 200 | Time 6.647(6.253) | Loss -20.096(-17.195) | NFE Forward 62(51.1) | NFE Backward 63(62.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 210 | Time 6.161(6.241) | Loss -21.085(-18.320) | NFE Forward 50(51.0) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 220 | Time 6.172(6.226) | Loss -21.727(-19.544) | NFE Forward 50(50.9) | NFE Backward 63(63.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 230 | Time 6.215(6.312) | Loss -22.236(-20.440) | NFE Forward 50(53.0) | NFE Backward 63(63.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 240 | Time 6.184(6.299) | Loss -23.326(-21.392) | NFE Forward 50(52.8) | NFE Backward 63(63.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 250 | Time 6.739(6.360) | Loss -23.431(-22.344) | NFE Forward 62(53.5) | NFE Backward 63(63.5) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -24.967 | NFE 51 | NoImproveEpochs 02/16
Epoch 0 | Iter 260 | Time 6.246(6.995) | Loss -25.529(-23.280) | NFE Forward 50(53.2) | NFE Backward 63(63.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 270 | Time 6.438(6.860) | Loss -26.051(-24.109) | NFE Forward 56(55.9) | NFE Backward 63(63.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 280 | Time 6.656(6.769) | Loss -27.116(-24.931) | NFE Forward 62(57.6) | NFE Backward 63(63.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 290 | Time 6.619(6.720) | Loss -28.518(-25.934) | NFE Forward 62(59.0) | NFE Backward 63(63.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 300 | Time 6.629(6.782) | Loss -26.513(-26.711) | NFE Forward 62(60.0) | NFE Backward 63(64.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 310 | Time 7.062(6.850) | Loss -28.876(-27.130) | NFE Forward 62(60.7) | NFE Backward 69(65.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 320 | Time 6.686(6.788) | Loss -29.513(-27.773) | NFE Forward 62(61.1) | NFE Backward 63(65.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 330 | Time 6.986(6.818) | Loss -31.186(-28.505) | NFE Forward 62(61.4) | NFE Backward 69(65.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 340 | Time 7.003(6.876) | Loss -30.261(-29.041) | NFE Forward 62(61.6) | NFE Backward 69(66.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 350 | Time 6.989(6.906) | Loss -30.842(-29.462) | NFE Forward 62(61.7) | NFE Backward 69(67.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 360 | Time 6.578(6.896) | Loss -31.793(-30.106) | NFE Forward 62(61.8) | NFE Backward 63(67.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 370 | Time 7.035(6.929) | Loss -33.320(-30.885) | NFE Forward 62(61.9) | NFE Backward 69(67.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 380 | Time 6.963(6.949) | Loss -33.203(-31.521) | NFE Forward 62(61.9) | NFE Backward 69(68.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 390 | Time 6.997(7.001) | Loss -31.975(-31.925) | NFE Forward 62(62.0) | NFE Backward 69(68.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 400 | Time 7.050(7.018) | Loss -32.617(-32.266) | NFE Forward 62(62.0) | NFE Backward 69(69.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 410 | Time 7.010(7.011) | Loss -33.487(-32.684) | NFE Forward 62(62.0) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 420 | Time 6.973(7.006) | Loss -34.678(-33.014) | NFE Forward 62(62.0) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 430 | Time 6.985(7.001) | Loss -34.977(-33.424) | NFE Forward 62(62.0) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 440 | Time 6.961(7.005) | Loss -34.070(-33.784) | NFE Forward 62(62.0) | NFE Backward 69(69.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 450 | Time 6.990(7.015) | Loss -34.858(-34.081) | NFE Forward 62(62.0) | NFE Backward 69(69.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 460 | Time 7.040(7.018) | Loss -35.970(-34.423) | NFE Forward 62(62.0) | NFE Backward 69(69.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 470 | Time 6.991(7.030) | Loss -35.828(-34.804) | NFE Forward 62(62.0) | NFE Backward 69(69.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 480 | Time 7.012(7.043) | Loss -35.264(-35.006) | NFE Forward 62(62.0) | NFE Backward 69(69.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 490 | Time 7.015(7.041) | Loss -35.351(-35.255) | NFE Forward 62(62.0) | NFE Backward 69(69.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 500 | Time 7.047(7.068) | Loss -35.497(-35.416) | NFE Forward 62(62.0) | NFE Backward 69(69.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 510 | Time 7.024(7.092) | Loss -35.548(-35.403) | NFE Forward 62(62.0) | NFE Backward 69(70.1) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -35.329 | NFE 62 | NoImproveEpochs 03/16
Epoch 0 | Iter 520 | Time 6.995(7.785) | Loss -36.167(-35.533) | NFE Forward 62(62.0) | NFE Backward 69(70.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 530 | Time 7.054(7.538) | Loss -36.283(-35.829) | NFE Forward 62(62.0) | NFE Backward 69(70.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 540 | Time 7.397(7.448) | Loss -36.154(-36.021) | NFE Forward 62(62.0) | NFE Backward 75(70.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 550 | Time 6.991(7.342) | Loss -36.639(-36.186) | NFE Forward 62(62.0) | NFE Backward 69(70.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 560 | Time 7.037(7.276) | Loss -36.656(-36.352) | NFE Forward 62(62.0) | NFE Backward 69(70.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 570 | Time 7.400(7.288) | Loss -36.482(-36.325) | NFE Forward 62(62.0) | NFE Backward 75(71.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 580 | Time 7.021(7.232) | Loss -36.420(-36.479) | NFE Forward 62(62.0) | NFE Backward 69(71.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 590 | Time 7.024(7.177) | Loss -35.942(-36.581) | NFE Forward 62(62.0) | NFE Backward 69(70.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 600 | Time 7.453(7.199) | Loss -36.162(-36.587) | NFE Forward 62(62.0) | NFE Backward 75(71.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 610 | Time 7.016(7.166) | Loss -37.504(-36.813) | NFE Forward 62(62.0) | NFE Backward 69(70.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 620 | Time 7.423(7.134) | Loss -36.496(-37.095) | NFE Forward 62(62.0) | NFE Backward 75(70.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 630 | Time 7.014(7.194) | Loss -37.743(-37.153) | NFE Forward 62(62.0) | NFE Backward 69(71.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 640 | Time 7.405(7.192) | Loss -37.872(-37.326) | NFE Forward 62(62.0) | NFE Backward 75(71.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 650 | Time 7.354(7.206) | Loss -37.162(-37.336) | NFE Forward 62(62.0) | NFE Backward 75(71.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 660 | Time 7.380(7.205) | Loss -36.765(-37.495) | NFE Forward 62(62.0) | NFE Backward 75(71.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 670 | Time 7.377(7.259) | Loss -38.350(-37.543) | NFE Forward 62(62.0) | NFE Backward 75(72.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 680 | Time 7.463(7.256) | Loss -37.470(-37.608) | NFE Forward 62(62.0) | NFE Backward 75(72.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 690 | Time 7.374(7.278) | Loss -37.450(-37.701) | NFE Forward 62(62.0) | NFE Backward 75(73.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 700 | Time 7.374(7.297) | Loss -38.274(-37.727) | NFE Forward 62(62.0) | NFE Backward 75(73.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 710 | Time 7.353(7.275) | Loss -38.044(-37.856) | NFE Forward 62(62.0) | NFE Backward 75(73.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 720 | Time 7.375(7.316) | Loss -37.747(-37.762) | NFE Forward 62(62.0) | NFE Backward 75(73.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 730 | Time 7.015(7.272) | Loss -38.595(-37.895) | NFE Forward 62(62.0) | NFE Backward 69(73.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 740 | Time 6.970(7.246) | Loss -38.906(-38.031) | NFE Forward 62(62.0) | NFE Backward 69(72.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 750 | Time 7.401(7.230) | Loss -37.484(-38.192) | NFE Forward 62(62.0) | NFE Backward 75(72.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 760 | Time 7.398(7.245) | Loss -38.974(-38.343) | NFE Forward 62(62.0) | NFE Backward 75(72.8) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -38.314 | NFE 62 | NoImproveEpochs 04/16
Epoch 0 | Iter 770 | Time 6.946(8.082) | Loss -38.401(-38.517) | NFE Forward 62(62.0) | NFE Backward 69(72.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 780 | Time 7.389(7.849) | Loss -38.805(-38.441) | NFE Forward 62(62.0) | NFE Backward 75(73.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 790 | Time 7.013(7.645) | Loss -39.159(-38.535) | NFE Forward 62(62.0) | NFE Backward 69(73.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 800 | Time 7.416(7.564) | Loss -39.419(-38.617) | NFE Forward 62(62.0) | NFE Backward 75(73.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 810 | Time 7.422(7.509) | Loss -38.441(-38.707) | NFE Forward 62(62.0) | NFE Backward 75(74.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 820 | Time 7.005(7.441) | Loss -39.445(-38.763) | NFE Forward 62(62.0) | NFE Backward 69(74.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 830 | Time 7.448(7.380) | Loss -39.782(-39.084) | NFE Forward 62(62.0) | NFE Backward 75(73.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 840 | Time 7.355(7.338) | Loss -38.417(-39.199) | NFE Forward 62(62.0) | NFE Backward 75(73.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 850 | Time 7.366(7.346) | Loss -38.127(-39.192) | NFE Forward 62(62.0) | NFE Backward 75(73.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 860 | Time 7.383(7.363) | Loss -39.538(-39.161) | NFE Forward 62(62.0) | NFE Backward 75(74.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 870 | Time 7.397(7.378) | Loss -38.604(-39.237) | NFE Forward 62(62.0) | NFE Backward 75(74.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 880 | Time 7.417(7.391) | Loss -39.087(-39.314) | NFE Forward 62(62.0) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 890 | Time 7.389(7.396) | Loss -38.855(-39.324) | NFE Forward 62(62.0) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 900 | Time 7.457(7.398) | Loss -39.958(-39.397) | NFE Forward 62(62.0) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 910 | Time 7.379(7.392) | Loss -39.504(-39.326) | NFE Forward 62(61.8) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 920 | Time 7.417(7.397) | Loss -39.656(-39.369) | NFE Forward 62(61.9) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=0.01, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 930 | Time 7.403(7.371) | Loss -39.898(-39.694) | NFE Forward 62(61.9) | NFE Backward 75(74.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 940 | Time 7.411(7.360) | Loss -40.026(-39.787) | NFE Forward 62(61.9) | NFE Backward 75(74.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 950 | Time 7.464(7.377) | Loss -40.212(-39.869) | NFE Forward 62(62.0) | NFE Backward 75(74.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 960 | Time 7.511(7.387) | Loss -40.031(-39.952) | NFE Forward 62(62.0) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 970 | Time 7.379(7.379) | Loss -39.431(-39.981) | NFE Forward 62(62.0) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 980 | Time 7.383(7.390) | Loss -39.189(-39.925) | NFE Forward 62(62.0) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 990 | Time 7.001(7.380) | Loss -39.903(-40.005) | NFE Forward 62(62.0) | NFE Backward 69(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1000 | Time 7.429(7.390) | Loss -39.882(-39.991) | NFE Forward 62(62.0) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1010 | Time 7.415(7.372) | Loss -41.058(-40.224) | NFE Forward 62(62.0) | NFE Backward 75(74.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1020 | Time 7.402(7.330) | Loss -40.274(-40.487) | NFE Forward 62(62.0) | NFE Backward 75(73.8) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -39.711 | NFE 62 | NoImproveEpochs 05/16
Epoch 0 | Iter 1030 | Time 7.385(8.084) | Loss -40.734(-40.515) | NFE Forward 62(62.0) | NFE Backward 75(74.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1040 | Time 7.420(7.857) | Loss -40.829(-40.599) | NFE Forward 62(62.0) | NFE Backward 75(74.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1050 | Time 7.432(7.709) | Loss -41.259(-40.682) | NFE Forward 62(62.0) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1060 | Time 7.455(7.612) | Loss -40.087(-40.667) | NFE Forward 62(62.0) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1070 | Time 7.370(7.539) | Loss -41.485(-40.793) | NFE Forward 62(62.0) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1080 | Time 7.381(7.495) | Loss -41.484(-40.831) | NFE Forward 62(62.0) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1090 | Time 7.421(7.470) | Loss -40.927(-40.973) | NFE Forward 62(62.0) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1100 | Time 7.367(7.443) | Loss -41.624(-40.969) | NFE Forward 62(62.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1110 | Time 7.416(7.431) | Loss -41.141(-40.924) | NFE Forward 62(62.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1120 | Time 7.460(7.429) | Loss -40.551(-41.015) | NFE Forward 62(62.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1130 | Time 7.377(7.437) | Loss -41.434(-41.125) | NFE Forward 62(62.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1140 | Time 7.387(7.429) | Loss -41.923(-41.113) | NFE Forward 62(62.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1150 | Time 7.390(7.417) | Loss -41.473(-41.262) | NFE Forward 62(62.1) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1160 | Time 7.429(7.440) | Loss -40.726(-41.170) | NFE Forward 62(62.7) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1170 | Time 7.503(7.437) | Loss -39.979(-41.100) | NFE Forward 62(62.6) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1180 | Time 7.402(7.425) | Loss -42.451(-41.380) | NFE Forward 62(62.4) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=0.01, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 6.592(6.592) | Loss 37.451(37.451) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000) | JFrobint: 0.00096351
Epoch 0 | Iter 1190 | Time 7.386(7.419) | Loss -41.306(-41.505) | NFE Forward 62(62.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1200 | Time 7.628(7.451) | Loss -41.188(-41.439) | NFE Forward 68(63.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1210 | Time 7.377(7.441) | Loss -41.727(-41.345) | NFE Forward 62(63.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1220 | Time 7.393(7.435) | Loss -41.222(-41.215) | NFE Forward 62(62.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1230 | Time 7.373(7.421) | Loss -41.786(-41.355) | NFE Forward 62(62.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1240 | Time 7.403(7.416) | Loss -42.332(-41.608) | NFE Forward 62(62.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=0.01, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 6.704(6.704) | Loss 37.833(37.833) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000) | JFrobint: 0.00114287
Epoch 0 | Iter 1250 | Time 7.360(7.399) | Loss -42.255(-41.684) | NFE Forward 62(62.3) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1260 | Time 7.367(7.408) | Loss -41.264(-41.609) | NFE Forward 62(62.4) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=0.01, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=1024, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=1024, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 6.531(6.531) | Loss 37.552(37.552) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000) | JFrobint: 0.00106994
[VAL] Epoch 0 | Val Loss 37.125 | NFE 20 | NoImproveEpochs 01/16
Epoch 0 | Iter 1270 | Time 7.369(7.408) | Loss -41.697(-41.682) | NFE Forward 62(62.2) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1280 | Time 7.373(7.409) | Loss -42.148(-41.791) | NFE Forward 62(62.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -41.696 | NFE 62 | NoImproveEpochs 06/16
Epoch 0 | Iter 1290 | Time 7.363(8.032) | Loss -42.094(-41.983) | NFE Forward 62(62.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1300 | Time 7.428(7.837) | Loss -42.687(-42.020) | NFE Forward 62(62.4) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1310 | Time 7.981(7.727) | Loss -42.298(-42.247) | NFE Forward 68(62.9) | NFE Backward 81(75.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1320 | Time 7.571(7.648) | Loss -43.141(-42.352) | NFE Forward 68(63.4) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1330 | Time 7.567(7.604) | Loss -41.965(-42.337) | NFE Forward 68(63.8) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1340 | Time 7.399(7.558) | Loss -43.838(-42.408) | NFE Forward 62(63.8) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1350 | Time 7.416(7.559) | Loss -42.903(-42.480) | NFE Forward 62(63.8) | NFE Backward 75(75.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1360 | Time 7.602(7.547) | Loss -42.255(-42.457) | NFE Forward 68(64.0) | NFE Backward 75(75.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1370 | Time 7.449(7.505) | Loss -42.338(-42.533) | NFE Forward 62(63.3) | NFE Backward 75(75.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1380 | Time 7.579(7.493) | Loss -42.345(-42.614) | NFE Forward 68(63.5) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=0.01, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=512, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=512, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 5.580(5.580) | Loss 37.610(37.610) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000) | JFrobint: 0.00104062
[VAL] Epoch 0 | Val Loss 37.104 | NFE 20 | NoImproveEpochs 01/16
Epoch 0 | Iter 1390 | Time 7.497(7.497) | Loss -42.509(-42.638) | NFE Forward 62(63.8) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 10 | Time 4.979(5.551) | Loss 35.532(37.063) | NFE Forward 20(20.0) | NFE Backward 27(22.7) | CNF Time 1.000(1.000) | JFrobint: 0.00288682
Epoch 0 | Iter 1400 | Time 7.584(7.517) | Loss -42.310(-42.650) | NFE Forward 68(64.4) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 20 | Time 6.474(5.557) | Loss 34.650(36.401) | NFE Forward 26(22.0) | NFE Backward 33(24.4) | CNF Time 1.000(1.000) | JFrobint: 0.00926626
Epoch 0 | Iter 30 | Time 6.108(5.760) | Loss 32.449(35.445) | NFE Forward 26(23.3) | NFE Backward 33(27.3) | CNF Time 1.000(1.000) | JFrobint: 0.03150891
Epoch 0 | Iter 1410 | Time 7.419(7.521) | Loss -43.230(-42.657) | NFE Forward 62(64.8) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1420 | Time 7.412(7.514) | Loss -42.893(-42.720) | NFE Forward 62(64.7) | NFE Backward 75(75.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1430 | Time 8.079(7.510) | Loss -42.250(-42.800) | NFE Forward 68(64.2) | NFE Backward 81(75.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1440 | Time 7.473(7.500) | Loss -43.835(-42.949) | NFE Forward 62(64.5) | NFE Backward 75(75.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1450 | Time 7.434(7.490) | Loss -43.691(-43.231) | NFE Forward 62(64.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1460 | Time 7.384(7.509) | Loss -43.995(-43.353) | NFE Forward 62(64.1) | NFE Backward 75(75.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1470 | Time 7.604(7.568) | Loss -43.040(-43.315) | NFE Forward 68(64.6) | NFE Backward 75(76.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1480 | Time 7.354(7.637) | Loss -43.587(-43.350) | NFE Forward 62(65.1) | NFE Backward 75(76.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1490 | Time 7.410(7.644) | Loss -42.444(-43.198) | NFE Forward 62(65.2) | NFE Backward 75(76.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1500 | Time 8.035(7.629) | Loss -43.871(-43.275) | NFE Forward 68(65.6) | NFE Backward 81(76.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1510 | Time 7.668(7.612) | Loss -43.762(-43.323) | NFE Forward 68(65.3) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1520 | Time 7.379(7.578) | Loss -43.685(-43.569) | NFE Forward 62(64.6) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1530 | Time 7.588(7.564) | Loss -44.101(-43.672) | NFE Forward 68(64.7) | NFE Backward 75(76.1) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -43.226 | NFE 66 | NoImproveEpochs 07/16
Epoch 0 | Iter 1540 | Time 7.383(8.431) | Loss -44.565(-43.776) | NFE Forward 62(65.0) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1550 | Time 7.662(8.179) | Loss -43.192(-43.780) | NFE Forward 68(65.8) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1560 | Time 7.180(7.958) | Loss -43.702(-43.846) | NFE Forward 56(64.9) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1570 | Time 7.203(7.797) | Loss -44.390(-43.754) | NFE Forward 56(64.3) | NFE Backward 75(76.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1580 | Time 7.177(7.667) | Loss -44.906(-44.099) | NFE Forward 56(63.2) | NFE Backward 75(75.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1590 | Time 7.392(7.658) | Loss -43.817(-44.035) | NFE Forward 62(64.0) | NFE Backward 75(76.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1600 | Time 8.010(7.710) | Loss -43.179(-43.859) | NFE Forward 68(65.4) | NFE Backward 81(76.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1610 | Time 7.161(7.715) | Loss -43.817(-43.657) | NFE Forward 56(65.8) | NFE Backward 75(77.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1620 | Time 7.198(7.639) | Loss -44.062(-43.752) | NFE Forward 56(65.3) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1630 | Time 7.562(7.594) | Loss -43.292(-43.667) | NFE Forward 68(64.6) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1640 | Time 7.540(7.566) | Loss -43.478(-43.862) | NFE Forward 68(64.5) | NFE Backward 75(76.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1650 | Time 8.013(7.578) | Loss -44.227(-43.952) | NFE Forward 68(64.7) | NFE Backward 81(76.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1660 | Time 7.581(7.628) | Loss -44.405(-44.189) | NFE Forward 68(65.4) | NFE Backward 75(76.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1670 | Time 7.166(7.650) | Loss -44.246(-44.307) | NFE Forward 56(65.0) | NFE Backward 75(77.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1680 | Time 7.150(7.687) | Loss -44.319(-44.312) | NFE Forward 56(65.5) | NFE Backward 75(77.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1690 | Time 7.210(7.668) | Loss -44.971(-44.248) | NFE Forward 56(64.9) | NFE Backward 75(77.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1700 | Time 7.606(7.605) | Loss -44.515(-44.475) | NFE Forward 68(63.5) | NFE Backward 75(77.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1710 | Time 7.639(7.534) | Loss -44.360(-44.628) | NFE Forward 68(62.6) | NFE Backward 75(76.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1720 | Time 7.979(7.547) | Loss -45.119(-44.641) | NFE Forward 68(62.9) | NFE Backward 81(76.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1730 | Time 8.002(7.614) | Loss -44.795(-44.606) | NFE Forward 68(63.8) | NFE Backward 81(77.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1740 | Time 7.161(7.540) | Loss -45.420(-44.696) | NFE Forward 56(62.9) | NFE Backward 75(76.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1750 | Time 7.174(7.463) | Loss -45.484(-44.827) | NFE Forward 56(61.3) | NFE Backward 75(76.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1760 | Time 7.196(7.410) | Loss -45.690(-45.014) | NFE Forward 56(59.6) | NFE Backward 75(76.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1770 | Time 7.616(7.427) | Loss -46.464(-45.146) | NFE Forward 56(58.8) | NFE Backward 81(77.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1780 | Time 7.615(7.486) | Loss -44.439(-45.142) | NFE Forward 56(58.3) | NFE Backward 81(78.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1790 | Time 8.039(7.551) | Loss -45.107(-45.105) | NFE Forward 68(59.5) | NFE Backward 81(78.5) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -44.582 | NFE 62 | NoImproveEpochs 08/16
Epoch 0 | Iter 1800 | Time 7.635(8.187) | Loss -45.910(-45.139) | NFE Forward 56(58.7) | NFE Backward 81(78.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1810 | Time 7.568(7.950) | Loss -46.504(-45.318) | NFE Forward 56(58.6) | NFE Backward 81(78.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1820 | Time 7.131(7.788) | Loss -44.794(-45.314) | NFE Forward 56(57.9) | NFE Backward 75(78.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1830 | Time 7.577(7.749) | Loss -45.806(-45.408) | NFE Forward 56(58.4) | NFE Backward 81(79.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1840 | Time 7.587(7.697) | Loss -43.985(-45.371) | NFE Forward 68(58.9) | NFE Backward 75(79.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1850 | Time 7.197(7.595) | Loss -45.930(-45.145) | NFE Forward 56(59.1) | NFE Backward 75(78.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1860 | Time 7.997(7.612) | Loss -45.084(-45.125) | NFE Forward 68(60.4) | NFE Backward 81(78.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1870 | Time 7.206(7.538) | Loss -46.248(-45.185) | NFE Forward 56(60.1) | NFE Backward 75(77.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1880 | Time 7.192(7.471) | Loss -46.568(-45.352) | NFE Forward 56(59.1) | NFE Backward 75(77.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1890 | Time 7.591(7.459) | Loss -45.929(-45.569) | NFE Forward 56(58.1) | NFE Backward 81(77.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1900 | Time 7.595(7.510) | Loss -46.745(-45.640) | NFE Forward 56(57.8) | NFE Backward 81(78.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1910 | Time 7.623(7.499) | Loss -44.927(-45.585) | NFE Forward 56(57.5) | NFE Backward 81(78.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1920 | Time 7.239(7.493) | Loss -45.710(-45.748) | NFE Forward 56(57.0) | NFE Backward 75(78.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1930 | Time 7.576(7.486) | Loss -45.932(-45.862) | NFE Forward 56(56.7) | NFE Backward 81(78.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1940 | Time 7.588(7.500) | Loss -46.013(-45.965) | NFE Forward 56(57.2) | NFE Backward 81(78.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1950 | Time 7.582(7.502) | Loss -45.383(-46.019) | NFE Forward 56(57.0) | NFE Backward 81(79.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1960 | Time 8.007(7.570) | Loss -45.721(-45.916) | NFE Forward 68(57.9) | NFE Backward 81(79.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1970 | Time 7.819(7.613) | Loss -45.472(-45.793) | NFE Forward 62(58.6) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1980 | Time 7.607(7.577) | Loss -45.810(-46.015) | NFE Forward 56(57.9) | NFE Backward 81(79.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1990 | Time 7.188(7.550) | Loss -46.631(-46.237) | NFE Forward 56(57.3) | NFE Backward 75(79.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2000 | Time 7.609(7.555) | Loss -47.299(-46.249) | NFE Forward 56(56.8) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2010 | Time 7.623(7.575) | Loss -46.329(-46.358) | NFE Forward 56(57.0) | NFE Backward 81(80.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2020 | Time 7.809(7.601) | Loss -45.897(-46.435) | NFE Forward 62(57.0) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2030 | Time 7.599(7.634) | Loss -46.323(-46.356) | NFE Forward 56(57.5) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2040 | Time 7.615(7.599) | Loss -47.351(-46.303) | NFE Forward 56(58.0) | NFE Backward 81(79.9) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -45.701 | NFE 56 | NoImproveEpochs 09/16
Epoch 0 | Iter 2050 | Time 7.647(8.356) | Loss -47.094(-46.415) | NFE Forward 56(57.5) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2060 | Time 7.606(8.080) | Loss -47.302(-46.618) | NFE Forward 56(57.2) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2070 | Time 7.614(7.928) | Loss -46.230(-46.399) | NFE Forward 56(57.8) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2080 | Time 7.582(7.836) | Loss -47.385(-46.345) | NFE Forward 56(58.5) | NFE Backward 81(79.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2090 | Time 7.158(7.690) | Loss -47.358(-46.536) | NFE Forward 56(57.8) | NFE Backward 75(79.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2100 | Time 7.552(7.643) | Loss -46.553(-46.653) | NFE Forward 56(57.6) | NFE Backward 81(79.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2110 | Time 7.566(7.639) | Loss -45.330(-46.525) | NFE Forward 56(58.0) | NFE Backward 81(79.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2120 | Time 7.589(7.665) | Loss -47.313(-46.749) | NFE Forward 56(58.5) | NFE Backward 81(80.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2130 | Time 7.597(7.640) | Loss -46.839(-46.714) | NFE Forward 56(58.1) | NFE Backward 81(80.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2140 | Time 7.845(7.625) | Loss -47.047(-46.785) | NFE Forward 62(57.8) | NFE Backward 81(80.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2150 | Time 7.616(7.632) | Loss -46.731(-46.943) | NFE Forward 56(58.0) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2160 | Time 7.585(7.622) | Loss -47.010(-47.110) | NFE Forward 56(58.3) | NFE Backward 81(80.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2170 | Time 7.814(7.646) | Loss -46.926(-47.174) | NFE Forward 62(59.2) | NFE Backward 81(80.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2180 | Time 7.782(7.674) | Loss -47.790(-47.207) | NFE Forward 62(59.3) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2190 | Time 7.595(7.685) | Loss -46.860(-47.093) | NFE Forward 56(59.6) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2200 | Time 7.596(7.689) | Loss -47.480(-47.173) | NFE Forward 56(59.8) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2210 | Time 7.570(7.672) | Loss -47.501(-47.271) | NFE Forward 56(59.5) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2220 | Time 7.609(7.668) | Loss -47.703(-47.272) | NFE Forward 56(59.3) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2230 | Time 7.824(7.692) | Loss -47.151(-47.298) | NFE Forward 62(59.9) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2240 | Time 7.816(7.695) | Loss -48.321(-47.297) | NFE Forward 62(60.0) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2250 | Time 7.290(7.663) | Loss -47.184(-47.438) | NFE Forward 56(59.9) | NFE Backward 75(79.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2260 | Time 7.830(7.670) | Loss -47.702(-47.468) | NFE Forward 62(60.0) | NFE Backward 81(79.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2270 | Time 7.835(7.714) | Loss -47.482(-47.334) | NFE Forward 62(60.7) | NFE Backward 81(80.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2280 | Time 7.816(7.737) | Loss -47.875(-47.338) | NFE Forward 62(60.7) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2290 | Time 7.204(7.715) | Loss -47.385(-47.466) | NFE Forward 56(60.1) | NFE Backward 75(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2300 | Time 7.819(7.716) | Loss -48.539(-47.733) | NFE Forward 62(59.7) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -47.463 | NFE 62 | NoImproveEpochs 10/16
Epoch 0 | Iter 2310 | Time 7.595(8.465) | Loss -48.150(-47.917) | NFE Forward 56(59.9) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2320 | Time 7.801(8.234) | Loss -48.010(-47.752) | NFE Forward 62(60.2) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2330 | Time 7.794(8.075) | Loss -47.639(-47.805) | NFE Forward 62(60.4) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2340 | Time 7.779(7.960) | Loss -48.218(-47.737) | NFE Forward 62(60.6) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2350 | Time 7.814(7.900) | Loss -47.393(-47.712) | NFE Forward 62(60.8) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2360 | Time 7.851(7.828) | Loss -47.455(-47.789) | NFE Forward 62(60.6) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2370 | Time 7.628(7.799) | Loss -48.502(-47.892) | NFE Forward 56(60.5) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2380 | Time 7.771(7.796) | Loss -47.495(-47.883) | NFE Forward 62(60.8) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2390 | Time 7.593(7.790) | Loss -49.455(-47.974) | NFE Forward 56(61.0) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2400 | Time 7.801(7.764) | Loss -49.335(-48.175) | NFE Forward 62(60.7) | NFE Backward 81(80.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2410 | Time 7.837(7.775) | Loss -48.015(-48.264) | NFE Forward 62(61.0) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2420 | Time 7.831(7.776) | Loss -48.311(-48.277) | NFE Forward 62(60.9) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2430 | Time 7.794(7.788) | Loss -49.015(-48.279) | NFE Forward 62(61.3) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2440 | Time 7.797(7.789) | Loss -47.230(-48.243) | NFE Forward 62(61.3) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2450 | Time 7.432(7.786) | Loss -47.672(-48.312) | NFE Forward 62(61.5) | NFE Backward 75(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2460 | Time 7.801(7.766) | Loss -48.809(-48.240) | NFE Forward 62(61.7) | NFE Backward 81(80.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2470 | Time 7.871(7.749) | Loss -49.493(-48.327) | NFE Forward 62(61.6) | NFE Backward 81(80.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2480 | Time 7.786(7.767) | Loss -49.001(-48.572) | NFE Forward 62(61.6) | NFE Backward 81(80.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2490 | Time 7.830(7.777) | Loss -49.549(-48.737) | NFE Forward 62(61.5) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2500 | Time 7.803(7.786) | Loss -49.189(-48.583) | NFE Forward 62(61.7) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2510 | Time 7.842(7.798) | Loss -46.846(-48.392) | NFE Forward 62(61.8) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2520 | Time 7.812(7.802) | Loss -48.940(-48.504) | NFE Forward 62(61.9) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2530 | Time 7.806(7.812) | Loss -48.015(-48.457) | NFE Forward 62(61.9) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2540 | Time 7.834(7.815) | Loss -48.453(-48.537) | NFE Forward 62(61.9) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2550 | Time 7.852(7.818) | Loss -49.010(-48.714) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2560 | Time 7.839(7.814) | Loss -48.680(-48.721) | NFE Forward 62(61.8) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -48.069 | NFE 62 | NoImproveEpochs 11/16
Epoch 0 | Iter 2570 | Time 7.789(8.422) | Loss -48.000(-48.772) | NFE Forward 62(61.9) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2580 | Time 7.772(8.211) | Loss -49.127(-48.782) | NFE Forward 62(61.9) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2590 | Time 7.784(8.075) | Loss -49.430(-48.899) | NFE Forward 62(61.9) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2600 | Time 7.764(7.988) | Loss -48.895(-49.055) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2610 | Time 7.819(7.924) | Loss -50.400(-49.089) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2620 | Time 7.789(7.887) | Loss -49.802(-48.895) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2630 | Time 7.803(7.860) | Loss -49.784(-48.954) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2640 | Time 7.884(7.847) | Loss -49.414(-49.082) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2650 | Time 7.776(7.828) | Loss -48.459(-49.039) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2660 | Time 7.791(7.818) | Loss -50.113(-49.275) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2670 | Time 7.818(7.818) | Loss -48.614(-49.217) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2680 | Time 7.784(7.817) | Loss -49.805(-49.215) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2690 | Time 7.796(7.815) | Loss -48.518(-49.327) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2700 | Time 7.820(7.815) | Loss -48.886(-49.439) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2710 | Time 7.818(7.797) | Loss -49.679(-49.332) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2720 | Time 7.794(7.802) | Loss -49.662(-49.496) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2730 | Time 7.793(7.791) | Loss -49.181(-49.384) | NFE Forward 62(62.0) | NFE Backward 81(80.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2740 | Time 7.810(7.795) | Loss -50.233(-49.652) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2750 | Time 7.812(7.801) | Loss -49.616(-49.680) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2760 | Time 7.833(7.804) | Loss -49.676(-49.748) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2770 | Time 7.822(7.801) | Loss -50.013(-49.830) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2780 | Time 7.850(7.804) | Loss -49.379(-49.767) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2790 | Time 7.782(7.805) | Loss -50.568(-49.899) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2800 | Time 7.840(7.809) | Loss -50.592(-50.092) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2810 | Time 7.788(7.811) | Loss -50.240(-49.963) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -48.488 | NFE 62 | NoImproveEpochs 12/16
Epoch 0 | Iter 2820 | Time 7.819(8.600) | Loss -50.994(-49.925) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2830 | Time 7.804(8.326) | Loss -50.851(-49.995) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2840 | Time 7.774(8.145) | Loss -49.829(-49.950) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2850 | Time 7.794(8.031) | Loss -49.782(-49.897) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2860 | Time 7.812(7.954) | Loss -50.610(-50.128) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2870 | Time 7.806(7.901) | Loss -50.690(-50.263) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2880 | Time 7.802(7.871) | Loss -50.628(-50.400) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2890 | Time 7.784(7.844) | Loss -50.850(-50.454) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2900 | Time 7.800(7.829) | Loss -50.061(-50.383) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2910 | Time 7.811(7.823) | Loss -50.275(-50.460) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2920 | Time 7.794(7.816) | Loss -48.261(-50.254) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2930 | Time 7.779(7.814) | Loss -50.532(-50.037) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2940 | Time 7.873(7.811) | Loss -50.510(-49.955) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2950 | Time 7.819(7.808) | Loss -51.876(-50.165) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2960 | Time 7.817(7.804) | Loss -50.801(-50.298) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2970 | Time 7.813(7.802) | Loss -50.870(-50.419) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2980 | Time 7.804(7.808) | Loss -50.403(-50.409) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 2990 | Time 7.801(7.805) | Loss -51.418(-50.542) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3000 | Time 7.888(7.806) | Loss -50.634(-50.604) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3010 | Time 7.777(7.802) | Loss -50.768(-50.591) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3020 | Time 7.817(7.804) | Loss -52.550(-50.804) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3030 | Time 7.809(7.804) | Loss -51.743(-50.924) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3040 | Time 7.787(7.802) | Loss -50.368(-50.902) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3050 | Time 7.765(7.802) | Loss -51.190(-50.998) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3060 | Time 7.797(7.803) | Loss -51.661(-51.221) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3070 | Time 7.782(7.803) | Loss -50.979(-51.106) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -49.108 | NFE 62 | NoImproveEpochs 13/16
Epoch 0 | Iter 3080 | Time 7.780(8.474) | Loss -50.351(-50.665) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3090 | Time 7.741(8.246) | Loss -51.290(-50.720) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3100 | Time 7.849(8.098) | Loss -51.626(-50.893) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3110 | Time 7.820(8.002) | Loss -50.732(-51.023) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3120 | Time 7.842(7.939) | Loss -49.354(-50.993) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3130 | Time 7.773(7.897) | Loss -50.970(-50.843) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3140 | Time 7.831(7.865) | Loss -50.637(-51.023) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3150 | Time 7.821(7.851) | Loss -50.657(-51.082) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3160 | Time 7.777(7.835) | Loss -51.929(-51.319) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 3170 | Time 7.852(7.830) | Loss -52.156(-51.432) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 0 | Time 8.024(7.843) | Loss -51.240(-51.369) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 1 | Val Loss -50.789 | NFE 62 | NoImproveEpochs 14/16
Epoch 1 | Iter 10 | Time 7.778(8.455) | Loss -51.439(-51.439) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 20 | Time 7.775(8.238) | Loss -52.142(-51.477) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 30 | Time 7.766(8.093) | Loss -52.844(-51.545) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 40 | Time 7.768(7.996) | Loss -50.569(-51.557) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 50 | Time 7.792(7.934) | Loss -52.549(-51.623) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 60 | Time 7.820(7.891) | Loss -51.891(-51.543) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 70 | Time 7.834(7.864) | Loss -49.955(-51.551) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 80 | Time 7.808(7.848) | Loss -51.148(-51.485) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 90 | Time 7.796(7.834) | Loss -51.417(-51.552) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 100 | Time 7.876(7.830) | Loss -51.503(-51.193) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 110 | Time 7.814(7.827) | Loss -53.132(-51.384) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 120 | Time 7.851(7.818) | Loss -51.531(-51.447) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 130 | Time 7.855(7.812) | Loss -52.676(-51.482) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 140 | Time 7.860(7.817) | Loss -53.528(-51.693) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 150 | Time 7.780(7.812) | Loss -52.486(-51.826) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 160 | Time 7.761(7.812) | Loss -52.324(-51.861) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 170 | Time 7.755(7.809) | Loss -52.193(-51.840) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 180 | Time 7.836(7.807) | Loss -51.288(-51.500) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 190 | Time 7.841(7.804) | Loss -51.472(-51.658) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 200 | Time 7.824(7.805) | Loss -52.163(-51.895) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 210 | Time 7.819(7.803) | Loss -52.069(-51.963) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 220 | Time 7.903(7.808) | Loss -51.751(-52.127) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 230 | Time 7.802(7.806) | Loss -52.753(-52.046) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 240 | Time 7.836(7.813) | Loss -51.464(-52.058) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 250 | Time 7.814(7.829) | Loss -52.110(-51.840) | NFE Forward 62(62.0) | NFE Backward 81(81.2) | CNF Time 1.000(1.000)
[VAL] Epoch 1 | Val Loss -49.807 | NFE 62 | NoImproveEpochs 15/16
Epoch 1 | Iter 260 | Time 7.815(8.617) | Loss -52.189(-51.840) | NFE Forward 62(62.0) | NFE Backward 81(81.2) | CNF Time 1.000(1.000)
Epoch 1 | Iter 270 | Time 7.867(8.343) | Loss -52.879(-52.093) | NFE Forward 62(62.0) | NFE Backward 81(81.1) | CNF Time 1.000(1.000)
Epoch 1 | Iter 280 | Time 7.791(8.160) | Loss -51.995(-52.098) | NFE Forward 62(62.0) | NFE Backward 81(81.1) | CNF Time 1.000(1.000)
Epoch 1 | Iter 290 | Time 7.787(8.041) | Loss -53.282(-52.272) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 300 | Time 7.827(7.967) | Loss -52.681(-52.374) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 310 | Time 7.827(7.919) | Loss -53.074(-52.645) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 320 | Time 7.795(7.880) | Loss -50.931(-52.379) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 330 | Time 7.791(7.864) | Loss -52.150(-51.988) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 340 | Time 7.829(7.842) | Loss -52.571(-51.939) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 350 | Time 7.804(7.814) | Loss -53.143(-52.063) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 1 | Iter 360 | Time 7.805(7.818) | Loss -52.721(-52.323) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 370 | Time 7.858(7.823) | Loss -52.411(-52.593) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 380 | Time 7.810(7.820) | Loss -52.312(-52.631) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 390 | Time 7.794(7.822) | Loss -54.181(-52.823) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 400 | Time 7.811(7.817) | Loss -53.775(-53.006) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 410 | Time 7.388(7.795) | Loss -51.958(-52.741) | NFE Forward 62(62.0) | NFE Backward 75(80.7) | CNF Time 1.000(1.000)
Epoch 1 | Iter 420 | Time 7.817(7.801) | Loss -52.769(-52.848) | NFE Forward 62(62.0) | NFE Backward 81(80.8) | CNF Time 1.000(1.000)
Epoch 1 | Iter 430 | Time 7.804(7.806) | Loss -52.652(-52.913) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 440 | Time 7.802(7.804) | Loss -52.274(-52.826) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 450 | Time 7.836(7.814) | Loss -51.764(-52.708) | NFE Forward 62(62.0) | NFE Backward 81(80.9) | CNF Time 1.000(1.000)
Epoch 1 | Iter 460 | Time 7.842(7.813) | Loss -53.208(-52.960) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 470 | Time 7.770(7.807) | Loss -52.476(-52.947) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 480 | Time 7.813(7.809) | Loss -52.700(-52.797) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 490 | Time 7.857(7.806) | Loss -52.846(-52.834) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 500 | Time 7.824(7.813) | Loss -52.781(-52.949) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 510 | Time 7.847(7.814) | Loss -53.698(-53.159) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 1 | Val Loss -51.234 | NFE 62 | NoImproveEpochs 16/16
Epoch 1 | Iter 520 | Time 7.765(8.485) | Loss -52.771(-53.246) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 530 | Time 7.817(8.257) | Loss -53.126(-53.116) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 540 | Time 7.772(8.103) | Loss -54.022(-53.316) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 550 | Time 7.810(8.006) | Loss -53.443(-53.413) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 560 | Time 7.765(7.937) | Loss -51.420(-53.241) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 570 | Time 7.793(7.892) | Loss -52.541(-53.255) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 580 | Time 7.850(7.871) | Loss -53.351(-53.268) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 590 | Time 7.770(7.851) | Loss -53.011(-53.328) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 600 | Time 7.787(7.834) | Loss -52.834(-53.439) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 610 | Time 7.813(7.826) | Loss -52.494(-53.401) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 620 | Time 7.817(7.822) | Loss -53.299(-53.407) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 630 | Time 7.767(7.815) | Loss -53.591(-53.558) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 640 | Time 7.779(7.815) | Loss -54.098(-53.380) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 650 | Time 7.762(7.808) | Loss -52.970(-53.248) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 660 | Time 7.810(7.808) | Loss -52.959(-53.379) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 670 | Time 7.811(7.810) | Loss -52.664(-53.439) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 680 | Time 7.800(7.808) | Loss -53.777(-53.273) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 690 | Time 7.902(7.812) | Loss -54.372(-53.498) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 700 | Time 7.779(7.810) | Loss -53.863(-53.681) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 710 | Time 7.798(7.808) | Loss -55.036(-53.984) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 720 | Time 7.830(7.808) | Loss -53.591(-53.942) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 730 | Time 7.844(7.810) | Loss -54.605(-53.909) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 740 | Time 7.823(7.811) | Loss -53.401(-53.768) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 750 | Time 7.791(7.811) | Loss -53.027(-53.614) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
Epoch 1 | Iter 760 | Time 7.735(7.803) | Loss -53.632(-53.774) | NFE Forward 62(62.0) | NFE Backward 81(81.0) | CNF Time 1.000(1.000)
[VAL] Epoch 1 | Val Loss -52.142 | NFE 62 | NoImproveEpochs 17/16
Training has finished.
/data/cephfs/punim0011/jtan/github/ffjord/cnf_flow.py
Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, atol=1e-05, batch_norm=False, batch_size=512, bn_lag=0, dataset='custom', dims='64-64-64', divergence_fn='brute_force', dl2int=None, early_stopping=16, evaluate=False, gpu=0, hdim_factor=10, l1int=None, l2int=None, layer_type='concatsquash', log_freq=10, lr=0.001, multigpu=False, n_epochs=32, nhidden=1, nonlinearity='tanh', num_blocks=1, rademacher=False, residual=False, resume=None, rtol=1e-05, save='experiments/cnf', solver='dopri5', spectral_norm=False, step_size=None, test_atol=None, test_batch_size=512, test_rtol=None, test_solver=None, time_length=1.0, train_T=True, val_freq=256, viz_freq=100, weight_decay=1e-05)
Note: NumExpr detected 24 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
NumExpr defaulting to 8 threads.
chain.0.sqrt_end_time
chain.0.odefunc.odefunc._num_evals
chain.0.odefunc.odefunc.diffeq.layers.0._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.0._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.0._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.1._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.1._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.1._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.2._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.2._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.2._hyper_gate.bias
chain.0.odefunc.odefunc.diffeq.layers.3._layer.weight
chain.0.odefunc.odefunc.diffeq.layers.3._layer.bias
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_bias.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.weight
chain.0.odefunc.odefunc.diffeq.layers.3._hyper_gate.bias
SequentialFlow(
  (chain): ModuleList(
    (0): CNF(
      (odefunc): RegularizedODEfunc(
        (odefunc): ODEfunc(
          (diffeq): ODEnet(
            (layers): ModuleList(
              (0): ConcatSquashLinear(
                (_layer): Linear(in_features=36, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (1): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (2): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=256, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=256, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=256, bias=True)
              )
              (3): ConcatSquashLinear(
                (_layer): Linear(in_features=256, out_features=36, bias=True)
                (_hyper_bias): Linear(in_features=1, out_features=36, bias=False)
                (_hyper_gate): Linear(in_features=1, out_features=36, bias=True)
              )
            )
            (activation_fns): ModuleList(
              (0): Tanh()
              (1): Tanh()
              (2): Tanh()
            )
          )
        )
      )
    )
  )
)
Using 4 GPUs.
Number of trainable parameters: 152721
Epoch 0 | Iter 0 | Time 5.601(5.601) | Loss 37.442(37.442) | NFE Forward 20(20.0) | NFE Backward 21(21.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss 37.061 | NFE 20 | NoImproveEpochs 00/16
Epoch 0 | Iter 10 | Time 2.510(4.758) | Loss 35.568(36.963) | NFE Forward 20(20.0) | NFE Backward 27(22.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 20 | Time 2.746(4.059) | Loss 34.711(36.339) | NFE Forward 26(21.5) | NFE Backward 27(24.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 30 | Time 3.101(3.718) | Loss 32.472(35.400) | NFE Forward 26(23.0) | NFE Backward 33(26.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 40 | Time 3.308(3.463) | Loss 27.359(33.494) | NFE Forward 32(24.5) | NFE Backward 33(27.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 50 | Time 3.716(3.552) | Loss 20.955(30.223) | NFE Forward 32(27.0) | NFE Backward 39(31.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 60 | Time 4.788(3.829) | Loss 14.040(25.766) | NFE Forward 38(29.1) | NFE Backward 51(37.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 70 | Time 4.797(4.155) | Loss 7.859(20.600) | NFE Forward 38(32.1) | NFE Backward 51(41.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 80 | Time 5.345(4.522) | Loss 3.828(15.555) | NFE Forward 44(35.0) | NFE Backward 57(46.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 90 | Time 5.648(4.851) | Loss 0.928(10.959) | NFE Forward 50(39.3) | NFE Backward 57(50.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 100 | Time 5.613(5.099) | Loss -2.195(6.835) | NFE Forward 50(42.5) | NFE Backward 57(52.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 110 | Time 5.620(5.294) | Loss -3.349(3.391) | NFE Forward 50(45.0) | NFE Backward 57(54.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 120 | Time 5.645(5.421) | Loss -5.854(0.369) | NFE Forward 50(46.7) | NFE Backward 57(55.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 130 | Time 6.017(5.611) | Loss -8.649(-2.356) | NFE Forward 50(47.8) | NFE Backward 63(57.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 140 | Time 5.981(5.741) | Loss -10.106(-4.689) | NFE Forward 50(48.5) | NFE Backward 63(59.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 150 | Time 5.992(5.794) | Loss -11.948(-6.746) | NFE Forward 50(49.0) | NFE Backward 63(60.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 160 | Time 5.968(5.856) | Loss -14.077(-8.743) | NFE Forward 50(49.4) | NFE Backward 63(61.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 170 | Time 5.979(5.910) | Loss -14.152(-10.585) | NFE Forward 50(49.6) | NFE Backward 63(62.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 180 | Time 5.972(5.961) | Loss -15.913(-11.984) | NFE Forward 50(49.7) | NFE Backward 63(62.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 190 | Time 5.963(5.962) | Loss -16.229(-13.457) | NFE Forward 50(49.8) | NFE Backward 63(62.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 200 | Time 6.357(5.980) | Loss -16.776(-14.862) | NFE Forward 50(49.9) | NFE Backward 69(63.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 210 | Time 6.339(6.045) | Loss -16.974(-15.815) | NFE Forward 50(49.9) | NFE Backward 69(64.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 220 | Time 5.976(6.017) | Loss -19.959(-16.929) | NFE Forward 50(49.9) | NFE Backward 63(63.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 230 | Time 5.953(5.998) | Loss -20.391(-18.106) | NFE Forward 50(50.0) | NFE Backward 63(63.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 240 | Time 7.203(6.163) | Loss -20.615(-19.078) | NFE Forward 62(51.3) | NFE Backward 75(65.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 250 | Time 5.948(6.115) | Loss -22.907(-19.946) | NFE Forward 50(50.9) | NFE Backward 63(64.9) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -21.335 | NFE 50 | NoImproveEpochs 00/16
Epoch 0 | Iter 260 | Time 5.877(6.782) | Loss -23.168(-20.576) | NFE Forward 50(51.1) | NFE Backward 63(65.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 270 | Time 5.871(6.479) | Loss -23.287(-21.561) | NFE Forward 50(50.7) | NFE Backward 63(64.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 280 | Time 6.302(6.357) | Loss -24.820(-22.265) | NFE Forward 62(51.6) | NFE Backward 63(64.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 290 | Time 5.893(6.209) | Loss -23.987(-22.936) | NFE Forward 50(51.1) | NFE Backward 63(64.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 300 | Time 6.246(6.112) | Loss -25.181(-23.713) | NFE Forward 50(50.7) | NFE Backward 69(64.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 310 | Time 7.059(6.142) | Loss -25.581(-24.417) | NFE Forward 62(52.0) | NFE Backward 75(64.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 320 | Time 5.846(6.301) | Loss -26.123(-24.776) | NFE Forward 50(53.3) | NFE Backward 63(66.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 330 | Time 6.607(6.321) | Loss -26.245(-25.247) | NFE Forward 56(53.2) | NFE Backward 69(66.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 340 | Time 5.999(6.299) | Loss -26.853(-25.751) | NFE Forward 50(52.8) | NFE Backward 63(66.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 350 | Time 6.186(6.237) | Loss -28.576(-26.521) | NFE Forward 56(53.4) | NFE Backward 63(65.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 360 | Time 7.194(6.416) | Loss -28.803(-27.148) | NFE Forward 62(56.1) | NFE Backward 75(66.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 370 | Time 7.224(6.556) | Loss -28.201(-27.555) | NFE Forward 62(57.7) | NFE Backward 75(67.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 380 | Time 6.415(6.568) | Loss -30.078(-28.137) | NFE Forward 62(58.2) | NFE Backward 63(67.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 390 | Time 7.170(6.610) | Loss -30.002(-28.608) | NFE Forward 62(58.9) | NFE Backward 75(67.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 400 | Time 6.407(6.622) | Loss -30.869(-29.186) | NFE Forward 62(59.7) | NFE Backward 63(67.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 410 | Time 6.209(6.645) | Loss -31.002(-29.640) | NFE Forward 56(59.5) | NFE Backward 63(68.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 420 | Time 6.996(6.790) | Loss -30.661(-29.884) | NFE Forward 56(59.7) | NFE Backward 75(70.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 430 | Time 6.604(6.856) | Loss -30.098(-30.142) | NFE Forward 56(58.7) | NFE Backward 69(71.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 440 | Time 7.197(6.848) | Loss -31.106(-30.406) | NFE Forward 62(58.4) | NFE Backward 75(71.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 450 | Time 6.395(6.796) | Loss -32.464(-30.882) | NFE Forward 62(58.5) | NFE Backward 63(70.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 460 | Time 7.066(6.867) | Loss -32.117(-31.351) | NFE Forward 62(59.3) | NFE Backward 75(71.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 470 | Time 6.986(6.886) | Loss -32.791(-31.684) | NFE Forward 56(58.6) | NFE Backward 75(72.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 480 | Time 7.013(6.774) | Loss -33.362(-32.166) | NFE Forward 56(58.3) | NFE Backward 75(70.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 490 | Time 6.866(6.870) | Loss -33.577(-32.448) | NFE Forward 56(58.7) | NFE Backward 75(72.2) | CNF Time 1.000(1.000)
Epoch 0 | Iter 500 | Time 6.805(6.918) | Loss -34.194(-32.843) | NFE Forward 62(59.3) | NFE Backward 69(72.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 510 | Time 6.807(6.947) | Loss -35.229(-33.277) | NFE Forward 62(59.2) | NFE Backward 69(73.1) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -34.644 | NFE 62 | NoImproveEpochs 00/16
Epoch 0 | Iter 520 | Time 6.953(7.629) | Loss -34.428(-33.592) | NFE Forward 56(59.1) | NFE Backward 75(73.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 530 | Time 7.063(7.431) | Loss -35.600(-33.963) | NFE Forward 62(59.3) | NFE Backward 75(73.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 540 | Time 6.854(7.268) | Loss -34.695(-34.354) | NFE Forward 56(59.4) | NFE Backward 75(74.1) | CNF Time 1.000(1.000)
Epoch 0 | Iter 550 | Time 7.084(7.165) | Loss -34.642(-34.670) | NFE Forward 62(59.1) | NFE Backward 75(74.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 560 | Time 7.059(7.108) | Loss -35.836(-34.970) | NFE Forward 62(59.3) | NFE Backward 75(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 570 | Time 7.078(7.057) | Loss -35.861(-35.074) | NFE Forward 62(59.1) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 580 | Time 6.993(7.014) | Loss -35.118(-35.039) | NFE Forward 56(58.4) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 590 | Time 6.607(6.996) | Loss -36.568(-35.222) | NFE Forward 56(57.6) | NFE Backward 69(74.6) | CNF Time 1.000(1.000)
Epoch 0 | Iter 600 | Time 6.833(7.028) | Loss -37.412(-35.562) | NFE Forward 62(58.3) | NFE Backward 69(74.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 610 | Time 7.017(7.033) | Loss -35.786(-35.815) | NFE Forward 56(57.9) | NFE Backward 75(74.7) | CNF Time 1.000(1.000)
Epoch 0 | Iter 620 | Time 7.000(7.048) | Loss -36.074(-35.955) | NFE Forward 56(58.1) | NFE Backward 75(74.8) | CNF Time 1.000(1.000)
Epoch 0 | Iter 630 | Time 7.285(7.051) | Loss -36.508(-36.145) | NFE Forward 62(58.0) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 640 | Time 6.991(7.048) | Loss -35.489(-36.300) | NFE Forward 56(57.9) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 650 | Time 6.996(7.071) | Loss -37.177(-36.462) | NFE Forward 56(58.5) | NFE Backward 75(74.9) | CNF Time 1.000(1.000)
Epoch 0 | Iter 660 | Time 6.971(7.079) | Loss -38.075(-36.610) | NFE Forward 56(58.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 670 | Time 6.958(7.068) | Loss -37.596(-36.916) | NFE Forward 56(58.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 680 | Time 7.014(7.063) | Loss -37.664(-37.003) | NFE Forward 56(58.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 690 | Time 6.987(7.062) | Loss -36.543(-37.022) | NFE Forward 56(58.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 700 | Time 7.229(7.099) | Loss -37.404(-36.998) | NFE Forward 62(59.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 710 | Time 7.164(7.089) | Loss -37.157(-36.965) | NFE Forward 62(59.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 720 | Time 6.944(7.048) | Loss -37.530(-37.180) | NFE Forward 56(58.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 730 | Time 6.956(7.044) | Loss -38.254(-37.333) | NFE Forward 56(58.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 740 | Time 7.085(7.075) | Loss -37.064(-37.126) | NFE Forward 62(59.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 750 | Time 6.968(7.024) | Loss -36.781(-37.036) | NFE Forward 56(58.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 760 | Time 6.976(7.013) | Loss -37.527(-37.122) | NFE Forward 56(57.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -37.736 | NFE 58 | NoImproveEpochs 00/16
Epoch 0 | Iter 770 | Time 6.973(7.786) | Loss -38.462(-37.340) | NFE Forward 56(57.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 780 | Time 7.170(7.560) | Loss -38.468(-37.672) | NFE Forward 62(58.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 790 | Time 7.163(7.425) | Loss -37.910(-37.893) | NFE Forward 62(59.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 800 | Time 7.198(7.324) | Loss -39.273(-37.961) | NFE Forward 62(59.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 810 | Time 6.843(7.194) | Loss -38.627(-38.148) | NFE Forward 56(58.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 820 | Time 7.059(7.097) | Loss -37.859(-38.100) | NFE Forward 62(58.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 830 | Time 6.855(7.051) | Loss -37.329(-38.039) | NFE Forward 56(58.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 840 | Time 6.830(7.007) | Loss -38.143(-38.088) | NFE Forward 56(58.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 850 | Time 6.967(6.962) | Loss -39.009(-38.305) | NFE Forward 56(57.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 860 | Time 7.166(6.981) | Loss -38.329(-38.511) | NFE Forward 62(57.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 870 | Time 7.212(7.030) | Loss -38.726(-38.503) | NFE Forward 62(58.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 880 | Time 7.070(7.036) | Loss -37.210(-38.273) | NFE Forward 62(59.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 890 | Time 6.978(7.024) | Loss -39.639(-38.396) | NFE Forward 56(58.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 900 | Time 6.961(7.014) | Loss -38.910(-38.626) | NFE Forward 56(57.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 910 | Time 6.984(7.022) | Loss -39.283(-38.843) | NFE Forward 56(57.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 920 | Time 6.969(7.035) | Loss -39.918(-38.940) | NFE Forward 56(58.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 930 | Time 6.981(7.028) | Loss -39.183(-39.153) | NFE Forward 56(57.8) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 940 | Time 6.882(6.994) | Loss -39.667(-39.123) | NFE Forward 56(57.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 950 | Time 7.004(7.004) | Loss -39.335(-39.284) | NFE Forward 56(57.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 960 | Time 6.971(6.999) | Loss -39.345(-39.397) | NFE Forward 56(57.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 970 | Time 6.906(6.993) | Loss -39.804(-39.467) | NFE Forward 56(57.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 980 | Time 6.932(6.970) | Loss -40.279(-39.447) | NFE Forward 56(57.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 990 | Time 6.953(6.961) | Loss -40.147(-39.593) | NFE Forward 56(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1000 | Time 6.974(6.972) | Loss -40.314(-39.658) | NFE Forward 56(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1010 | Time 6.978(6.979) | Loss -39.477(-39.749) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1020 | Time 6.969(6.979) | Loss -40.003(-39.752) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -39.912 | NFE 57 | NoImproveEpochs 00/16
Epoch 0 | Iter 1030 | Time 7.186(7.640) | Loss -40.360(-39.834) | NFE Forward 62(56.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1040 | Time 6.990(7.422) | Loss -40.343(-39.929) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1050 | Time 7.179(7.290) | Loss -40.531(-39.863) | NFE Forward 62(57.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1060 | Time 7.216(7.208) | Loss -40.267(-39.754) | NFE Forward 62(57.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1070 | Time 6.991(7.126) | Loss -39.634(-39.892) | NFE Forward 56(56.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1080 | Time 6.987(7.093) | Loss -40.079(-40.152) | NFE Forward 56(56.8) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1090 | Time 7.023(7.077) | Loss -39.478(-40.328) | NFE Forward 56(56.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1100 | Time 6.995(7.057) | Loss -41.247(-40.473) | NFE Forward 56(56.8) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1110 | Time 6.990(7.039) | Loss -39.988(-40.587) | NFE Forward 56(56.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1120 | Time 6.991(7.034) | Loss -41.319(-40.539) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1130 | Time 7.011(7.026) | Loss -40.590(-40.665) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1140 | Time 7.021(7.037) | Loss -40.809(-40.573) | NFE Forward 56(56.9) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1150 | Time 6.996(7.038) | Loss -41.039(-40.669) | NFE Forward 56(57.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1160 | Time 7.003(7.027) | Loss -41.116(-40.837) | NFE Forward 56(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1170 | Time 6.994(7.025) | Loss -41.226(-40.972) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1180 | Time 6.994(7.015) | Loss -41.310(-41.045) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1190 | Time 7.003(7.005) | Loss -41.201(-41.036) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1200 | Time 6.962(7.001) | Loss -40.462(-41.027) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1210 | Time 6.989(6.993) | Loss -42.140(-41.113) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1220 | Time 6.985(7.004) | Loss -41.621(-41.226) | NFE Forward 56(56.5) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1230 | Time 6.990(6.997) | Loss -41.215(-41.397) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1240 | Time 7.390(7.011) | Loss -40.460(-41.365) | NFE Forward 68(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1250 | Time 6.993(7.016) | Loss -41.862(-41.462) | NFE Forward 56(56.8) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1260 | Time 6.995(7.024) | Loss -41.032(-41.600) | NFE Forward 56(57.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1270 | Time 6.996(7.014) | Loss -41.326(-41.623) | NFE Forward 56(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1280 | Time 6.968(7.004) | Loss -41.582(-41.677) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -41.651 | NFE 56 | NoImproveEpochs 00/16
Epoch 0 | Iter 1290 | Time 6.997(7.544) | Loss -40.372(-41.604) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1300 | Time 6.976(7.356) | Loss -42.962(-41.665) | NFE Forward 56(56.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1310 | Time 6.999(7.231) | Loss -42.665(-41.883) | NFE Forward 56(56.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1320 | Time 6.984(7.131) | Loss -41.879(-41.927) | NFE Forward 56(56.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1330 | Time 7.001(7.087) | Loss -42.125(-42.033) | NFE Forward 56(56.1) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1340 | Time 6.996(7.057) | Loss -42.066(-42.086) | NFE Forward 56(56.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1350 | Time 6.986(7.033) | Loss -43.236(-42.127) | NFE Forward 56(56.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1360 | Time 6.985(7.016) | Loss -42.794(-42.235) | NFE Forward 56(56.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1370 | Time 6.975(7.010) | Loss -41.417(-42.268) | NFE Forward 56(56.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1380 | Time 7.001(7.006) | Loss -41.829(-42.163) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1390 | Time 6.984(7.017) | Loss -43.039(-42.167) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1400 | Time 6.964(6.998) | Loss -42.867(-42.170) | NFE Forward 56(56.4) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1410 | Time 6.950(6.992) | Loss -43.725(-42.464) | NFE Forward 56(56.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1420 | Time 7.037(7.014) | Loss -43.520(-42.655) | NFE Forward 56(56.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1430 | Time 7.005(7.033) | Loss -42.303(-42.543) | NFE Forward 56(57.0) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1440 | Time 6.986(7.036) | Loss -42.654(-42.574) | NFE Forward 56(57.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1450 | Time 6.867(7.023) | Loss -42.239(-42.676) | NFE Forward 56(57.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1460 | Time 6.976(6.976) | Loss -42.601(-42.759) | NFE Forward 56(56.8) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1470 | Time 7.005(6.982) | Loss -42.995(-42.838) | NFE Forward 56(56.7) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1480 | Time 7.080(7.002) | Loss -44.003(-43.075) | NFE Forward 62(57.3) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1490 | Time 7.095(7.009) | Loss -43.259(-43.214) | NFE Forward 62(58.2) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1500 | Time 6.930(7.010) | Loss -43.818(-43.363) | NFE Forward 56(58.6) | NFE Backward 75(75.0) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1510 | Time 7.508(7.044) | Loss -43.335(-43.409) | NFE Forward 62(59.1) | NFE Backward 81(75.4) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1520 | Time 7.185(7.070) | Loss -43.097(-43.314) | NFE Forward 62(59.8) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1530 | Time 7.144(7.084) | Loss -44.096(-43.410) | NFE Forward 62(59.9) | NFE Backward 75(75.2) | CNF Time 1.000(1.000)
[VAL] Epoch 0 | Val Loss -43.401 | NFE 62 | NoImproveEpochs 00/16
Epoch 0 | Iter 1540 | Time 7.228(7.890) | Loss -43.935(-43.587) | NFE Forward 62(60.4) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1550 | Time 7.618(7.663) | Loss -44.385(-43.539) | NFE Forward 62(60.6) | NFE Backward 81(75.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1560 | Time 7.190(7.513) | Loss -42.054(-43.485) | NFE Forward 62(60.9) | NFE Backward 75(75.5) | CNF Time 1.000(1.000)
Epoch 0 | Iter 1570 | Time 6.995(7.400) | Loss -43.238(-43.368) | NFE Forward 56(61.0) | NFE Backward 75(75.3) | CNF Time 1.000(1.000)
